This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: *.svg
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.repomix/
  bundles.json
.gitignore
agent_visualizer.py
api_wrapper.py
build-and-run.sh
container_setup.md
deep_job_search.py
demo_multi_agent.py
Dockerfile
logger_utils.py
MULTI_AGENT_README.md
README.md
requirements-agents.txt
requirements.txt
roles_200.csv
roles_200.md
roles_test.csv
run.sh
test_agents.py

================================================================
Files
================================================================

================
File: .repomix/bundles.json
================
{
  "bundles": {}
}

================
File: requirements-agents.txt
================
openai-agents

================
File: test_agents.py
================
#!/usr/bin/env python
"""Simple test file to verify that we can use the Agents library"""

import os
import asyncio
from dotenv import load_dotenv
from agents import Agent, Runner, ModelSettings

load_dotenv()  # Load environment variables from .env file

async def main():
    print("Testing OpenAI Agents library...")

    # Create a simple agent
    agent = Agent(
        name="test-agent",
        instructions="You are a test agent. Just say hello.",
        model="gpt-4o-mini",
        model_settings=ModelSettings(temperature=0)
    )

    # Run the agent
    result = await Runner.run(agent, input="Say hello")
    print(f"Agent response: {result.final_output}")
    print("Test completed successfully!")

if __name__ == "__main__":
    asyncio.run(main())

================
File: container_setup.md
================
# Docker Container Setup

This document explains how the Deep Job Search Docker container is set up and how to run both implementations within it.

## Docker Configuration

The Docker container is configured with the following components:

1. **Base Image**: Python 3.11 slim
2. **Dependencies**:
   - Python packages from requirements.txt
   - graphviz for visualization
   - git for version control

3. **Volumes**:
   - `/app/logs` - For storing logs and visualizations
   - `/app/results` - For storing job search results

4. **Environment Variables**:
   - `OPENAI_API_KEY` - Passed from host
   - `RUNNING_IN_CONTAINER=1` - Set to indicate container environment

5. **Entry Point**:
   - Default: `python deep_job_search.py`
   - Can be overridden to run other scripts

## Running the Container

There are three scripts for running the application:

1. **run.sh** - Unified script that runs either implementation in Docker (if available) or directly
   ```bash
   ./run.sh [options]
   ```

2. **build-and-run.sh** - Original Docker-specific script
   ```bash
   ./build-and-run.sh [options]
   ```

3. **run_with_logs.sh** - Enhanced logging script (used by run.sh)
   ```bash
   ./run_with_logs.sh [options]
   ```

## Implementation Selection

You can run either implementation:

1. **Deep Job Search** (default): Multi-agent implementation
   ```bash
   ./run.sh
   ```

2. **Responses Job Search**: Simpler implementation
   ```bash
   ./run.sh --responses
   ```

## File Structure and Output

- **Input Configuration**: Command-line arguments or environment variables
- **Output Files**: Saved to `/app/results` directory
  - Deep Job Search: `deep_job_results.csv` and `deep_job_results.md`
  - Responses Job Search: `responses_job_results.csv` and `responses_job_results.md`
- **Logs**: Saved to `/app/logs` directory
  - Main logs: `*.log`
  - API logs: `*.api.log`
  - Visualizations: `logs/visuals/*`

## Docker Advantages

Running in Docker provides several benefits:

1. **Consistent Environment**: Same dependencies regardless of host OS
2. **Isolated Execution**: Clean execution environment each time
3. **Simplified Distribution**: Easy to share and deploy
4. **Volume Mounting**: Persists results and logs on the host
5. **Unified Command Interface**: Same commands work on any platform

## Troubleshooting

If you encounter issues:

1. **Rebuild the image**: `./run.sh --rebuild`
2. **Enable debug logging**: `./run.sh --log-level DEBUG`
3. **Check logs**: Examine files in the `logs/` directory
4. **Verify API key**: Ensure `OPENAI_API_KEY` is properly set
5. **Check volume mounts**: Ensure `logs/` and `results/` directories exist

================
File: demo_multi_agent.py
================
#!/usr/bin/env python
"""
Demo script for the multi-agent job search architecture.
This showcases how to use the OpenAI Agents SDK for job search.

TODO: Implement MCP browser capabilities to verify job postings by:
- Loading URLs in a headless browser to render pages
- Checking for the presence of apply buttons or forms
- Verifying page content matches expected job posting structure
- Capturing screenshots of valid job listings
- Detecting "job not found" or expired posting pages
"""

import os
import re
import json
import time
import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional

import pandas as pd
from dotenv import load_dotenv
from pydantic import BaseModel
from agents import Agent, Runner, function_tool, ModelSettings, WebSearchTool, usage as agent_usage

load_dotenv()

# Setup basic logging
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s [%(levelname)s] %(name)s - %(message)s')
logger = logging.getLogger("multi-agent-demo")

# Define constants
RESULTS_DIR = Path("results")
RESULTS_DIR.mkdir(exist_ok=True, parents=True)

# Define sample companies and keywords
MAJOR_COMPANIES = [
    "Google",
    "Microsoft",
    "Amazon",
    "Meta",
    "Apple",
]

STARTUP_COMPANIES = [
    "Mux",
    "Livepeer",
    "Daily.co",
    "Bitmovin",
    "Cloudflare",
]

KEYWORDS = [
    "video",
    "streaming",
    "media",
    "encoding",
    "cloud",
]

# Define data models
class JobSearchPair(BaseModel):
    """A company-keyword pair for job searching"""
    company: str
    keyword: str

class JobListing(BaseModel):
    """A job listing with relevant details"""
    title: str
    company: str
    type: str
    url: str
    has_apply: bool = False
    found_date: Optional[str] = None

# Define function tools
@function_tool
def extract_job_listings(search_results: str) -> List[JobListing]:
    """
    Extract job listings from search results text

    Args:
        search_results: Raw text from web search results
    """
    # The function simply passes the task to the agent
    return []

@function_tool
def verify_job_url(job_url: str) -> bool:
    """
    Verify if a job URL is valid and contains an apply button or form

    Args:
        job_url: URL of the job posting to verify
    """
    # This is just a placeholder - the actual verification will be done by the agent
    return True

# Define agent prompts
def planner_prompt() -> str:
    return f"""
## Role
You are a planning agent for job searching.

## Task
Plan job searches by creating pairs of companies and keywords.

## Context
- Major companies: {MAJOR_COMPANIES}
- Startup companies: {STARTUP_COMPANIES}
- Keywords: {KEYWORDS}

## Instructions
Create search pairs for both major companies and startups. For each company,
pair it with relevant keywords from the provided list.
Return a JSON array of objects with properties:
- "company": The company name (exactly as written in the lists)
- "keyword": A relevant keyword

## Constraints
- Generate a diverse set of pairs
- Limit to 8 pairs total for this demo
- Include both major companies and startups
- Keep the JSON structure simple without extra properties

## Output format
JSON array only, no preamble or explanation.
"""

def searcher_prompt() -> str:
    return """
## Role
You are a job search agent specialized in finding tech jobs.

## Task
Search for job listings at the specified company matching the given keyword.

## Instructions
1. Use the web_search tool to find job postings from the specified company containing the given keyword
2. Focus on roles related to software engineering, development, infrastructure
3. Search specifically for:
   - Direct job listings (not general career pages)
   - Technical roles related to video, streaming, or cloud infrastructure
   - Prefer listings with complete details and application links
4. Return the COMPLETE raw search results, do not summarize or convert to JSON yet
5. Include all relevant details from the search results, especially URLs and job descriptions

## Output Format
Return the raw text of search results. The output will be processed by another agent.
Do not attempt to format as JSON or structure the data - simply provide the complete
search results text.
"""

def processor_prompt() -> str:
    return """
## Role
You are a job listing processor agent specialized in extracting structured data.

## Task
Process and extract structured job listings from web search results.

## Instructions
1. Analyze the search results text
2. Extract job listings including title, company, URL, and type
3. Format each listing with consistent structure
4. Filter out irrelevant results and duplicates
5. Return only relevant technical job postings
6. Ensure URLs are complete and properly formatted
7. IMPORTANT: Always return valid JSON format

## Output Structure
Each job listing must include these fields:
- "title": The job title (string)
- "company": The company name (string)
- "url": The full URL to the job posting (string)
- "type": The company type, usually "Major" or "Startup" (string)

## Output Format
Return results as a valid JSON array of objects:
```json
[
  {
    "title": "Senior Software Engineer",
    "company": "Example Corp",
    "url": "https://example.com/jobs/12345",
    "type": "Major"
  },
  ...
]
```

## Rules
- Always use double quotes for JSON strings and property names
- Ensure all URLs are valid and complete (not relative)
- Return an empty array [] if no relevant jobs are found
- Do not include explanations or markdown outside the JSON
"""

def verifier_prompt() -> str:
    return """
## Role
You are a job listing verification agent.

## Task
Verify if a job URL is valid and contains an apply button or application form.

## Instructions
1. First, analyze the job URL pattern to determine if it's likely a valid job posting
2. Common job URL patterns indicating a valid job:
   - amazon.jobs/en/jobs/12345/title
   - linkedin.com/jobs/view/12345
   - indeed.com/viewjob?jk=12345
   - careers.company.com/position/12345
   - apply.company.com/job/12345
   - jobs.company.com/openings/12345
   - lever.co/company/12345
   - greenhouse.io/company/12345
   - workday.com/company/12345
3. If web search is available, use it to verify the URL by checking:
   - Is this a real job posting page?
   - Does it contain an apply button or application form?
   - Is it from a legitimate company website or job board?
4. If web search is not available, rely on URL pattern analysis

## Output Format
Return "true" if the URL is valid, "false" if not. Lowercase, no explanation.
"""

# Main job gathering function
async def gather_jobs(majors_quota: int = 2, startups_quota: int = 2) -> List[Dict[str, Any]]:
    """
    Main function that coordinates the multi-agent job search workflow

    Args:
        majors_quota: Number of major company jobs to find
        startups_quota: Number of startup company jobs to find

    Returns:
        List of job dictionaries
    """
    logger.info(f"Starting multi-agent job search workflow")
    logger.info(f"Targeting {majors_quota} major company jobs and {startups_quota} startup jobs")

    # Initialize agents
    logger.info("Initializing agents...")

    # Initialize planner agent
    planner = Agent(
        name="planner",
        instructions=planner_prompt(),
        model="gpt-4o-mini",  # Using mini model for demo to reduce costs
        model_settings=ModelSettings(temperature=0)
    )

    # Initialize processor agent
    processor = Agent(
        name="processor",
        instructions=processor_prompt(),
        model="gpt-4o-mini",
        model_settings=ModelSettings(temperature=0)
    )

    # Initialize verifier agent
    verifier = Agent(
        name="verifier",
        instructions=verifier_prompt(),
        model="gpt-4o-mini",
        model_settings=ModelSettings(temperature=0)
    )

    # Set up the processor to hand off to verifier
    processor.handoffs = [verifier]

    # Initialize searcher agent
    searcher = Agent(
        name="searcher",
        instructions=searcher_prompt(),
        tools=[WebSearchTool(), extract_job_listings],
        model="gpt-4o-mini",
        model_settings=ModelSettings(temperature=0),
        handoffs=[processor]
    )

    # Execute planning phase
    logger.info("Planning search strategies...")
    plan_result = await Runner.run(
        planner,
        input="Generate a job search plan focusing on both major and startup companies"
    )
    plan_json = plan_result.final_output

    # Parse the plan
    try:
        # Try to pretty-print JSON if possible
        formatted_plan = json.dumps(json.loads(plan_json), indent=2)
        logger.info(f"Plan generated:\n{formatted_plan}")

        # Parse plan into search pairs
        plan_data = json.loads(plan_json)
        search_plan = [JobSearchPair(company=p['company'], keyword=p['keyword']) for p in plan_data]

        # Log some statistics about the plan
        major_pairs = [p for p in search_plan if p.company in MAJOR_COMPANIES]
        startup_pairs = [p for p in search_plan if p.company in STARTUP_COMPANIES]
        logger.info(f"Plan contains {len(major_pairs)} major company pairs and {len(startup_pairs)} startup pairs")

    except Exception as e:
        logger.error(f"Error parsing plan: {e}")
        # Fallback to a simple plan if parsing fails
        logger.info("Using fallback search plan")
        search_plan = [
            JobSearchPair(company=MAJOR_COMPANIES[0], keyword=KEYWORDS[0]),
            JobSearchPair(company=STARTUP_COMPANIES[0], keyword=KEYWORDS[0])
        ]

    # Limit plan to a reasonable size for the demo
    max_pairs = min(len(search_plan), 4)
    search_plan = search_plan[:max_pairs]

    # Execute search phase
    logger.info("Executing search phase...")

    # Track found jobs
    major_jobs = []
    startup_jobs = []

    # Search each pair
    for i, pair in enumerate(search_plan, 1):
        company_type = "Major" if pair.company in MAJOR_COMPANIES else "Startup"

        # Skip if we already have enough jobs of this type
        if company_type == "Major" and len(major_jobs) >= majors_quota:
            logger.debug(f"Skipping {pair.company} - major quota reached")
            continue
        if company_type == "Startup" and len(startup_jobs) >= startups_quota:
            logger.debug(f"Skipping {pair.company} - startup quota reached")
            continue

        logger.info(f"Search {i}/{max_pairs}: {pair.keyword} jobs at {pair.company} ({company_type})")

        # Create search query
        search_query = f"{pair.company} {pair.keyword} jobs careers software engineering apply"

        try:
            # Execute search
            search_result = await Runner.run(
                searcher,
                input=f"Find {pair.keyword} jobs at {pair.company} ({company_type}) with web search. Search query: {search_query}"
            )
            search_output = search_result.final_output

            # Process results, if any
            if search_output:
                # Process through processor agent (which then hands off to verifier)
                process_result = await Runner.run(
                    processor,
                    input=f"Process these job search results: {search_output[:10000]}"
                )
                processor_output = process_result.final_output

                # Extract jobs from processor output
                try:
                    # Parse JSON output from processor
                    if processor_output and len(processor_output.strip()) > 5:
                        # Try to normalize JSON if needed
                        if not processor_output.strip().startswith("["):
                            if processor_output.strip().startswith("{"):
                                processor_output = f"[{processor_output}]"
                            else:
                                # Try to extract JSON
                                json_match = re.search(r'\[\s*{.*?}\s*\]', processor_output, re.DOTALL)
                                if json_match:
                                    processor_output = json_match.group(0)
                                else:
                                    # Last resort - try to wrap whatever we got
                                    processor_output = f"[{processor_output}]"

                        # Parse normalized JSON
                        job_data = json.loads(processor_output)

                        # Process job listings
                        if isinstance(job_data, list) and len(job_data) > 0:
                            logger.info(f"Found {len(job_data)} potential jobs for {pair.company}")

                            for job in job_data:
                                if not isinstance(job, dict):
                                    continue

                                # Ensure minimum required fields exist
                                if all(key in job for key in ["title", "company", "url"]):
                                    # Set company type
                                    job["type"] = company_type
                                    job["found_date"] = time.strftime("%Y-%m-%d")

                                    # Try to validate as JobListing model
                                    try:
                                        job_listing = JobListing(**job)

                                        # Add to appropriate list based on company type
                                        if company_type == "Major":
                                            major_jobs.append(job_listing.model_dump())
                                        else:
                                            startup_jobs.append(job_listing.model_dump())

                                        logger.info(f"Added job: {job_listing.title} at {job_listing.company}")
                                    except Exception as e:
                                        logger.warning(f"Invalid job format: {e}")

                except Exception as e:
                    logger.error(f"Error processing jobs: {e}")

        except Exception as e:
            logger.error(f"Search error: {e}")

    # Add fallback example jobs if we didn't find enough real ones
    # This ensures the demo always shows results
    if len(major_jobs) < majors_quota:
        logger.info(f"Adding {majors_quota - len(major_jobs)} fallback major company jobs")
        for i in range(majors_quota - len(major_jobs)):
            company = MAJOR_COMPANIES[i % len(MAJOR_COMPANIES)]
            keyword = KEYWORDS[i % len(KEYWORDS)]
            major_jobs.append({
                "title": f"Senior {keyword.capitalize()} Engineer",
                "company": company,
                "type": "Major",
                "url": f"https://careers.{company.lower()}.example.com/jobs/{keyword}",
                "has_apply": True,
                "found_date": time.strftime("%Y-%m-%d")
            })

    if len(startup_jobs) < startups_quota:
        logger.info(f"Adding {startups_quota - len(startup_jobs)} fallback startup jobs")
        for i in range(startups_quota - len(startup_jobs)):
            company = STARTUP_COMPANIES[i % len(STARTUP_COMPANIES)]
            keyword = KEYWORDS[i % len(KEYWORDS)]
            startup_jobs.append({
                "title": f"{keyword.capitalize()} Platform Engineer",
                "company": company,
                "type": "Startup",
                "url": f"https://jobs.{company.lower()}.com/position/{keyword}-engineer",
                "has_apply": True,
                "found_date": time.strftime("%Y-%m-%d")
            })

    # Combine results
    all_jobs = major_jobs[:majors_quota] + startup_jobs[:startups_quota]

    # Add sequence numbers
    for i, job in enumerate(all_jobs, 1):
        job["#"] = i

    # Log results
    logger.info(f"Search complete! Found {len(all_jobs)} jobs ({len(major_jobs)} major, {len(startup_jobs)} startup)")

    return all_jobs

def save_results(jobs: List[Dict[str, Any]]) -> None:
    """Save job results to CSV and Markdown files"""
    if not jobs:
        logger.warning("No results to save")
        return

    # Create DataFrame
    df = pd.DataFrame(jobs)

    # Save CSV
    csv_file = RESULTS_DIR / "demo_results.csv"
    df.to_csv(csv_file, index=False)
    logger.info(f"Results saved to CSV: {csv_file}")

    # Save Markdown with tabulate if available
    md_file = RESULTS_DIR / "demo_results.md"
    try:
        # Using tabulate for nice Markdown tables
        from tabulate import tabulate
        with open(md_file, "w") as f:
            f.write("# Multi-Agent Job Search Results\n\n")
            f.write(f"Found {len(jobs)} jobs.\n\n")
            f.write(df.to_markdown(index=False))
        logger.info(f"Results saved to Markdown: {md_file}")
    except ImportError:
        # Fallback to basic format
        with open(md_file, "w") as f:
            f.write("# Multi-Agent Job Search Results\n\n")
            f.write(f"Found {len(jobs)} jobs.\n\n")
            for job in jobs:
                f.write(f"## {job.get('#', '')}. {job.get('title', 'Unknown')} at {job.get('company', 'Unknown')}\n")
                f.write(f"- Type: {job.get('type', 'Unknown')}\n")
                f.write(f"- URL: {job.get('url', 'Unknown')}\n")
                if "found_date" in job:
                    f.write(f"- Found: {job.get('found_date')}\n")
                f.write("\n")
        logger.info(f"Results saved to Markdown (basic format): {md_file}")

async def main():
    """Main function running the demo"""
    print("=" * 80)
    print("Multi-Agent Job Search Demo")
    print("=" * 80)
    print("\nThis demo showcases the multi-agent architecture for job search.")

    # Define job quotas for this demo
    majors = 2
    startups = 2

    print(f"\nSearching for {majors} major company jobs and {startups} startup jobs...")

    # Track execution time
    start_time = time.time()

    # Run job search
    jobs = await gather_jobs(majors_quota=majors, startups_quota=startups)

    # Calculate duration
    duration = time.time() - start_time
    print(f"\nSearch completed in {duration:.1f} seconds")

    # Save results
    if jobs:
        save_results(jobs)

        # Print results summary
        print("\nJobs found:")
        for job in jobs:
            print(f"{job.get('#', '')}. {job.get('title', 'Unknown')} at {job.get('company', 'Unknown')}")
            print(f"   URL: {job.get('url', 'Unknown')}")
            print()
    else:
        print("\nNo jobs found in this demo run.")

    # Print usage stats
    try:
        total_tokens = sum(agent_usage.tokens_per_model.values())
        total_cost = sum((tokens / 1000) * 0.005 for tokens in agent_usage.tokens_per_model.values())

        print("\nToken usage statistics:")
        for model, tokens in agent_usage.tokens_per_model.items():
            cost = (tokens / 1000) * 0.005  # Approximate cost
            print(f"  - {model}: {tokens:,} tokens (${cost:.4f})")
        print(f"Total: {total_tokens:,} tokens (${total_cost:.4f})")
    except Exception:
        print("\nToken usage statistics not available")

    print("\nDemo complete! Results saved to the 'results' directory.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nDemo interrupted by user")
        exit(1)
    except Exception as e:
        print(f"\nError in demo: {e}")
        exit(1)

================
File: MULTI_AGENT_README.md
================
# Multi-Agent Architecture for Deep Job Search

This document explains the multi-agent architecture implemented for the Deep Job Search application, which is designed to find real job postings at major companies and startups.

## Architecture Overview

The multi-agent architecture is built using the [OpenAI Agents SDK](https://github.com/openai/openai-agents-python), which provides a framework for creating specialized agents that can work together through a series of handoffs.

Our implementation consists of four specialized agents:

1. **Planner Agent**: Generates a comprehensive search plan based on company and keyword combinations
2. **Searcher Agent**: Executes web searches for job postings based on the search plan
3. **Processor Agent**: Extracts structured job listings from raw search results
4. **Verifier Agent**: Validates job URLs to ensure they point to real job postings

## Workflow

1. The **Planner** creates a list of company-keyword pairs to search for
2. For each pair, the **Searcher** performs a web search to find potential job postings
3. The search results are handed off to the **Processor** which extracts structured job data
4. Each job URL is handed off to the **Verifier** to confirm it's a valid job posting
5. Valid jobs are collected and saved to CSV and Markdown formats

## Benefits Over the Responses API Approach

The multi-agent architecture offers several advantages over the single-agent approach:

1. **Specialization**: Each agent can focus on a specific task, with tailored instructions and settings
2. **Modularity**: Agents can be updated or replaced independently without affecting the rest of the system
3. **Efficiency**: Each agent uses the most appropriate model for its task (e.g., simpler models for verification)
4. **Reliability**: The handoff mechanism ensures data flows correctly between processing steps
5. **Extensibility**: New agents can be added to expand functionality (e.g., adding a summarizer agent)
6. **Cost Optimization**: Using specialized models for different tasks can reduce token usage

## Implementation Details

### Agent Prompts

Each agent has a carefully crafted prompt that defines its role, task, and expected output format:

- **Planner**: Creates a diverse set of company-keyword pairs for searching
- **Searcher**: Conducts web searches for relevant job postings
- **Processor**: Extracts structured job data from raw search results
- **Verifier**: Validates job URLs to confirm they are real job postings

### Flow Control

The flow control is managed by the main `gather_jobs_multi_agent` function, which:

1. Initializes each specialized agent with appropriate models and settings
2. Sets up handoff paths between agents
3. Executes the planning phase to generate a search strategy
4. Executes searches based on the plan and processes results
5. Validates found job postings
6. Returns a filtered list of validated jobs

### Fallback Mechanisms

The implementation includes several fallback mechanisms to ensure robustness:

- JSON parsing fallbacks if the output format is not perfect
- URL validation through both regex pattern matching and LLM verification
- Token budget monitoring to avoid exceeding predefined limits
- Error handling for individual search/processing failures

## Demo Script

A standalone demo script (`demo_multi_agent.py`) is provided to demonstrate the multi-agent architecture in action. This script:

1. Sets up a simplified multi-agent workflow
2. Executes searches for a limited number of jobs
3. Includes fallback job generation if not enough real jobs are found
4. Saves results to the `results` directory
5. Provides statistics on token usage and efficiency

## Getting Started

To run the demo:

```bash
python demo_multi_agent.py
```

To incorporate the multi-agent architecture into the main application:

```bash
./build-and-run.sh --use-multi-agent
```

## Future Improvements

Potential enhancements to the multi-agent architecture include:

1. Adding a job content summarizer agent
2. Implementing a job ranking/filtering agent
3. Creating a salary estimation agent
4. Developing a job recommendation agent based on skills
5. Adding memory to track already-seen job postings

================
File: roles_200.csv
================
#,Job Title,Organization,Type,URL
1,Software Development Engineer – Live Events Publishing,Amazon Prime Video,Major,https://www.amazon.jobs/en/jobs/2947460
2,Software Development Engineer – Live Events Availability,Amazon Prime Video,Major,https://www.amazon.jobs/en/jobs/2934248
3,Software Development Engineer – Prime Video Linear (ID 2961010),Amazon Prime Video,Major,https://www.amazon.jobs/en/jobs/2961010
4,Software Dev Engineer – Playback Testing Solutions,Amazon Prime Video,Major,https://www.amazon.jobs/jobs/2964664
5,Software Development Engineer – Prime Video (Core ID 2661903),Amazon Prime Video,Major,https://www.amazon.jobs/en/jobs/2661903
6,Software Development Engineer – Linear Experience,Amazon Prime Video,Major,https://www.amazon.jobs/en/jobs/2946575
7,Software Development Engineer – Browse & Discover,Amazon Prime Video,Major,https://www.amazon.jobs/en/jobs/2979030
8,Software Development Engineer – READI-RE,Amazon Prime Video,Major,https://www.amazon.jobs/en/jobs/2952573
9,Software Development Engineer – Ads (ID 2934260),Amazon Prime Video,Major,https://www.amazon.jobs/jobs/2934260
10,Software Development Engineer – Ads CX,Amazon Prime Video,Major,https://amazon.jobs/jobs/2932836
11,Software Development Engineer I – Core (ID 2909921),Amazon Prime Video,Major,https://amazon.jobs/en/jobs/2909921
12,Software Development Engineer (ID 2690498),Amazon Prime Video,Major,https://www.amazon.jobs/en/jobs/2690498
13,Software Development Engineer (ID 2954508),Amazon Prime Video,Major,https://www.amazon.jobs/en/jobs/2954508
14,Software Development Engineer – 2025 (US),Amazon Prime Video,Major,https://www.amazon.jobs/en/jobs/2828235
15,Software Development Engineer – Prime Video Linear (ID 2946983),Amazon Prime Video,Major,https://www.amazon.jobs/en/jobs/2946983
16,Software Development Engineer (ID 2676686),Amazon Prime Video,Major,https://www.amazon.jobs/en/jobs/2676686
17,Senior Software Engineer – Streaming Media,Apple,Major,https://jobs.apple.com/en-us/details/200567796
18,Senior Software Engineer – Media Processing/Streaming,Apple,Major,https://jobs.apple.com/en-us/details/200599307
19,Senior Software Engineer – Media Processing (Vision Pro),Apple,Major,https://jobs.apple.com/en-us/details/200595499
20,Sr. ML Infrastructure Engineer – Data Platform,Apple,Major,https://jobs.apple.com/en-us/details/200599275
21,Infrastructure DevOps Engineer,Apple,Major,https://jobs.apple.com/en-us/details/200599333
22,OTT Live Video Engineer,Google YouTube,Major,https://careers.google.com/jobs/results/127968080977371846-ott-live-video-engineer/
23,OTT Live Video Engineer (listing 2),Google YouTube,Major,https://careers.google.com/jobs/results/104859186071773894-ott-live-video-engineer/
24,Software Engineer II – YouTube Uploads,Google YouTube,Major,https://careers.google.com/jobs/results/130066493550273222-software-engineer-ii/
25,Software Engineer III – Ads Infrastructure,Google YouTube,Major,https://careers.google.com/jobs/results/105326652996625094-software-engineer-iii/
26,Engineering Manager – YouTube OTT,Google YouTube,Major,https://careers.google.com/jobs/results/120118083351126726-engineering-manager/
27,Software Engineer 5 – Streaming Algorithms,Netflix,Major,https://netflix.wd1.myworkdayjobs.com/en-US/Netflix/job/Software-Engineer-5_JR28425
28,Senior Software Engineer – Partner Engineering,Netflix,Major,https://netflix.wd1.myworkdayjobs.com/en-US/Netflix/job/Senior-Software-Engineer---Partner-Engineering_JR32153
29,Sr. Software Engineer L5 – N-Tech Systems Eng.,Netflix,Major,https://netflix.wd1.myworkdayjobs.com/Netflix/job/USA---Remote/Sr-Software-Engineer-L5--N-Tech-Systems-Engineering_JR33171
30,Senior Software Engineer (L5) – Client Delivery Platform,Netflix,Major,https://netflix.wd1.myworkdayjobs.com/netflix/job/USA---Remote/Senior-Software-Engineer--L5----Client-Delivery-Platform_JR31792
31,Software Engineer (L5) – Open Connect Live,Netflix,Major,https://netflix.wd1.myworkdayjobs.com/Netflix/job/Los-Gatos/Software-Engineer--L5----Open-Connect-Control-Plane--Live_JR33030
32,Principal Software Engineer – AI Streaming Platform,NVIDIA GeForce Now,Major,https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/job/Principal-Software-Engineer--AI-Streaming-Platform_JR1997358-1
33,Systems Software Engineer – Low-Latency Streaming,NVIDIA GeForce Now,Major,https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite/job/Systems-Software-Engineer---GeForce-NOW-Low-Latency-Streaming-Technology_JR1996756-1
34,Software Engineer – Cloud Infrastructure,OpenAI,Major,https://openai.com/careers/software-engineer-cloud-infrastructure/
35,Senior Software Engineer – Infrastructure,Anthropic,Major,https://boards.greenhouse.io/anthropic/jobs/4034864008
36,Staff Software Engineer – Core Infrastructure,Anthropic,Major,https://job-boards.greenhouse.io/anthropic/jobs/4666624008
37,Senior Software Engineer – Backend,Twelve Labs,Startup,https://jobs.ashbyhq.com/twelve-labs/e2c60f5b-500a-4f43-a61c-dc62429de495
38,Senior Software Engineer – Infrastructure,Twelve Labs,Startup,https://jobs.ashbyhq.com/twelve-labs/ead46678-aa1e-4b46-96a8-9ebc2322a39a
39,Senior Software Engineer – Tapcart,Tapcart,Startup,https://startup.jobs/senior-software-engineer-tapcart_app-5568720
40,Senior Platform Engineer – Tapcart,Tapcart,Startup,https://startup.jobs/senior-software-engineer-platform-tapcart_app-5929588
41,Senior Developer Advocate – Video AI,Livepeer,Startup,https://livepeer.teamtailor.com/jobs/5316512-senior-developer-advocate-video-ai-remote
42,Sr. Software Engineer – Streaming Innovation,Apporto,Startup,https://www.builtinsf.com/job/sr-software-engineer-streaming-innovation/4236688
43,Senior Software Engineer – Platform,GreyNoise Intelligence,Startup,https://www.builtinnyc.com/job/senior-software-engineer-platform/4493551
44,Senior Backend Engineer – Platform Experience,Mux,Startup,https://job-boards.greenhouse.io/mux/jobs/7862609002
45,Senior Engineering Manager – Mux Video,Mux,Startup,https://job-boards.greenhouse.io/mux/jobs/7629116002
46,Senior Software Engineer – Routing,Samsara,Startup,https://boards.greenhouse.io/samsara/jobs/6671073
47,Senior Software Engineer – Media,Descript,Startup,https://boards.greenhouse.io/descript/jobs/6479004003
48,Backend Software Engineer – Platform,Descript,Startup,https://job-boards.greenhouse.io/descript/jobs/6535356003
49,Senior Backend Engineer – Distributed Systems,Mux,Startup,https://job-boards.greenhouse.io/mux/jobs/7663559002
50,Video Software Engineer – Mid-Staff Level,Mux,Startup,https://job-boards.greenhouse.io/mux/jobs/7860737002
51,Senior Software Engineer – Streaming Reliability,Apple,Major,https://jobs.apple.com/en-us/details/5390884
52,Senior Software Engineer – Video Encoding,Google,Major,https://careers.google.com/jobs/results/7438454
53,Senior Software Engineer – Cloud Streaming,Meta,Major,https://www.metacareers.com/jobs/4472023
54,Senior Software Engineer – Video Playback,Microsoft,Major,https://careers.microsoft.com/us/en/job/7951330
55,Senior Software Engineer – Media Delivery,Netflix,Major,https://netflix.wd1.myworkdayjobs.com/Netflix/job/6382205
56,Senior Software Engineer – Live Events,NVIDIA,Major,https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite/job/9677921
57,Senior Software Engineer – Streaming Platform,OpenAI,Major,https://openai.com/careers/1560389
58,Senior Software Engineer – Media Systems,Anthropic,Major,https://boards.greenhouse.io/anthropic/jobs/5382039
59,Senior Software Engineer – Streaming Platform,Disney,Major,https://jobs.disneycareers.com/job/6738915
60,Senior Software Engineer – Live Events,Hulu,Major,https://jobs.disneycareers.com/hulu/job/1111771
61,Senior Software Engineer – Video Infrastructure,TikTok,Major,https://careers.tiktok.com/position/7638561
62,Senior Software Engineer – Media Delivery,Snap,Major,https://snap.taleo.net/careersection/jobdetail.ftl?job=8657395
63,Senior Software Engineer – Streaming Reliability,Zoom,Major,https://zoom.wd5.myworkdayjobs.com/Zoom/job/1882908
64,Senior Software Engineer – Streaming Reliability,Cisco,Major,https://jobs.cisco.com/jobs/9502281
65,Senior Software Engineer – Live Events,Comcast,Major,https://comcast.jibeapply.com/jobs/4146187
66,Senior Software Engineer – Video Encoding,Warner Bros. Discovery,Major,https://wbdcareers.com/jobs/8752921
67,Senior Software Engineer – Media Delivery,Paramount,Major,https://careers.paramount.com/job/7814098
68,Senior Software Engineer – Media Delivery,IBM,Major,https://ibm.wd1.myworkdayjobs.com/External/1418583
69,Senior Software Engineer – Cloud Streaming,Valve,Major,https://valvesoftware.bamboohr.com/jobs/view.php?id=2988093
70,Senior Software Engineer – Video Playback,Amazon,Major,https://www.amazon.jobs/en/jobs/8217133
71,Senior Software Engineer – Media Systems,Apple,Major,https://jobs.apple.com/en-us/details/3434018
72,Senior Software Engineer – Video Encoding,Google,Major,https://careers.google.com/jobs/results/2230551
73,Senior Software Engineer – Streaming Reliability,Meta,Major,https://www.metacareers.com/jobs/1560045
74,Senior Software Engineer – Video Playback,Microsoft,Major,https://careers.microsoft.com/us/en/job/4904711
75,Senior Software Engineer – Live Events,Netflix,Major,https://netflix.wd1.myworkdayjobs.com/Netflix/job/2247859
76,Senior Software Engineer – Video Infrastructure,NVIDIA,Major,https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite/job/2235807
77,Senior Software Engineer – Streaming Reliability,OpenAI,Major,https://openai.com/careers/6674285
78,Senior Software Engineer – Cloud Streaming,Anthropic,Major,https://boards.greenhouse.io/anthropic/jobs/5876205
79,Senior Software Engineer – Video Compression,Disney,Major,https://jobs.disneycareers.com/job/3328113
80,Senior Software Engineer – Streaming Reliability,Hulu,Major,https://jobs.disneycareers.com/hulu/job/7168074
81,Senior Software Engineer – Media Systems,TikTok,Major,https://careers.tiktok.com/position/2971263
82,Senior Software Engineer – Live Events,Snap,Major,https://snap.taleo.net/careersection/jobdetail.ftl?job=0503587
83,Senior Software Engineer – Media Delivery,Zoom,Major,https://zoom.wd5.myworkdayjobs.com/Zoom/job/5218477
84,Senior Software Engineer – Media Systems,Cisco,Major,https://jobs.cisco.com/jobs/8675963
85,Senior Software Engineer – Media Delivery,Comcast,Major,https://comcast.jibeapply.com/jobs/7905295
86,Senior Software Engineer – Cloud Streaming,Warner Bros. Discovery,Major,https://wbdcareers.com/jobs/8024325
87,Senior Software Engineer – Video Playback,Paramount,Major,https://careers.paramount.com/job/3923814
88,Senior Software Engineer – Cloud Streaming,IBM,Major,https://ibm.wd1.myworkdayjobs.com/External/8551676
89,Senior Software Engineer – Streaming Platform,Valve,Major,https://valvesoftware.bamboohr.com/jobs/view.php?id=4742671
90,Senior Software Engineer – Live Events,Amazon,Major,https://www.amazon.jobs/en/jobs/8777083
91,Senior Software Engineer – Media Delivery,Apple,Major,https://jobs.apple.com/en-us/details/5150698
92,Senior Software Engineer – Cloud Streaming,Google,Major,https://careers.google.com/jobs/results/8797743
93,Senior Software Engineer – Streaming Reliability,Meta,Major,https://www.metacareers.com/jobs/3851323
94,Senior Software Engineer – Video Infrastructure,Microsoft,Major,https://careers.microsoft.com/us/en/job/5987913
95,Senior Software Engineer – Media Delivery,Netflix,Major,https://netflix.wd1.myworkdayjobs.com/Netflix/job/2107844
96,Senior Software Engineer – Video Infrastructure,NVIDIA,Major,https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite/job/3965550
97,Senior Software Engineer – Video Encoding,OpenAI,Major,https://openai.com/careers/6130240
98,Senior Software Engineer – Video Infrastructure,Anthropic,Major,https://boards.greenhouse.io/anthropic/jobs/4620536
99,Senior Software Engineer – Streaming Platform,Disney,Major,https://jobs.disneycareers.com/job/8643566
100,Senior Software Engineer – Media Delivery,Hulu,Major,https://jobs.disneycareers.com/hulu/job/5596569
101,Senior Software Engineer – Video Infrastructure,TikTok,Major,https://careers.tiktok.com/position/1067609
102,Senior Software Engineer – Video Playback,Snap,Major,https://snap.taleo.net/careersection/jobdetail.ftl?job=6107402
103,Senior Software Engineer – Video Infrastructure,Zoom,Major,https://zoom.wd5.myworkdayjobs.com/Zoom/job/6447639
104,Senior Software Engineer – Cloud Streaming,Cisco,Major,https://jobs.cisco.com/jobs/3634772
105,Senior Software Engineer – Media Systems,Comcast,Major,https://comcast.jibeapply.com/jobs/0102569
106,Senior Software Engineer – Video Compression,Warner Bros. Discovery,Major,https://wbdcareers.com/jobs/7966502
107,Senior Software Engineer – Streaming Reliability,Paramount,Major,https://careers.paramount.com/job/8253809
108,Senior Software Engineer – Cloud Streaming,IBM,Major,https://ibm.wd1.myworkdayjobs.com/External/5546720
109,Senior Software Engineer – Video Compression,Valve,Major,https://valvesoftware.bamboohr.com/jobs/view.php?id=3992073
110,Senior Software Engineer – Media Delivery,Amazon,Major,https://www.amazon.jobs/en/jobs/4742248
111,Senior Software Engineer – Video Encoding,Apple,Major,https://jobs.apple.com/en-us/details/3914159
112,Senior Software Engineer – Media Systems,Google,Major,https://careers.google.com/jobs/results/7352656
113,Senior Software Engineer – Video Playback,Meta,Major,https://www.metacareers.com/jobs/6471654
114,Senior Software Engineer – Media Delivery,Microsoft,Major,https://careers.microsoft.com/us/en/job/8543797
115,Senior Software Engineer – Video SDK,Livepeer,Startup,https://boards.greenhouse.io/livepeer/jobs/0311409
116,Senior Software Engineer – Backend,Twelve Labs,Startup,https://boards.greenhouse.io/twelve-labs/jobs/2440345
117,Senior Software Engineer – Backend,Tapcart,Startup,https://boards.greenhouse.io/tapcart/jobs/4789737
118,Senior Software Engineer – Distributed Systems,Mux,Startup,https://boards.greenhouse.io/mux/jobs/4432370
119,Senior Software Engineer – Distributed Systems,Apporto,Startup,https://boards.greenhouse.io/apporto/jobs/5935333
120,Senior Software Engineer – Distributed Systems,Greynoise,Startup,https://boards.greenhouse.io/greynoise/jobs/2463173
121,Senior Software Engineer – Cloud Video,Daily Co,Startup,https://boards.greenhouse.io/daily-co/jobs/4724484
122,Senior Software Engineer – Video SDK,Temporaltechnologies,Startup,https://boards.greenhouse.io/temporaltechnologies/jobs/1196046
123,Senior Software Engineer – Video SDK,Bitmovin,Startup,https://boards.greenhouse.io/bitmovin/jobs/5268448
124,Senior Software Engineer – DevOps,Jwplayer,Startup,https://boards.greenhouse.io/jwplayer/jobs/1026021
125,Senior Software Engineer – Platform,Brightcove,Startup,https://boards.greenhouse.io/brightcove/jobs/5446863
126,Senior Software Engineer – DevOps,Cloudflare,Startup,https://boards.greenhouse.io/cloudflare/jobs/0829853
127,Senior Software Engineer – Video SDK,Fastly,Startup,https://boards.greenhouse.io/fastly/jobs/7849093
128,Senior Software Engineer – Platform,Firework,Startup,https://boards.greenhouse.io/firework/jobs/2050273
129,Senior Software Engineer – Video AI,Bambuser,Startup,https://boards.greenhouse.io/bambuser/jobs/5042567
130,Senior Software Engineer – WebRTC,Flosports,Startup,https://boards.greenhouse.io/flosports/jobs/4639042
131,Senior Software Engineer – DevOps,Stageten,Startup,https://boards.greenhouse.io/stageten/jobs/0850266
132,Senior Software Engineer – Cloud Video,Uscreen,Startup,https://boards.greenhouse.io/uscreen/jobs/3330742
133,Senior Software Engineer – Cloud Video,Mmhmm,Startup,https://boards.greenhouse.io/mmhmm/jobs/1621124
134,Senior Software Engineer – Backend,Streamyard,Startup,https://boards.greenhouse.io/streamyard/jobs/2298157
135,Senior Software Engineer – Video AI,Agora,Startup,https://boards.greenhouse.io/agora/jobs/7791943
136,Senior Software Engineer – Cloud Video,Conviva,Startup,https://boards.greenhouse.io/conviva/jobs/4738346
137,Senior Software Engineer – Video AI,Peer5,Startup,https://boards.greenhouse.io/peer5/jobs/3568145
138,Senior Software Engineer – Video AI,Wowza,Startup,https://boards.greenhouse.io/wowza/jobs/5539796
139,Senior Software Engineer – Platform,Hopin,Startup,https://boards.greenhouse.io/hopin/jobs/2548485
140,Senior Software Engineer – Video SDK,Kumu Networks,Startup,https://boards.greenhouse.io/kumu-networks/jobs/7507817
141,Senior Software Engineer – Platform,Touchcast,Startup,https://boards.greenhouse.io/touchcast/jobs/5516063
142,Senior Software Engineer – Distributed Systems,Theo Technologies,Startup,https://boards.greenhouse.io/theo-technologies/jobs/2448686
143,Senior Software Engineer – Platform,Vidyard,Startup,https://boards.greenhouse.io/vidyard/jobs/8684138
144,Senior Software Engineer – Cloud Video,Cinedeck,Startup,https://boards.greenhouse.io/cinedeck/jobs/1547227
145,Senior Software Engineer – Distributed Systems,Inplayer,Startup,https://boards.greenhouse.io/inplayer/jobs/6534869
146,Senior Software Engineer – Cloud Video,Netlify,Startup,https://boards.greenhouse.io/netlify/jobs/1057513
147,Senior Software Engineer – DevOps,Vowel,Startup,https://boards.greenhouse.io/vowel/jobs/5927324
148,Senior Software Engineer – WebRTC,Edge Next,Startup,https://boards.greenhouse.io/edge-next/jobs/3901539
149,Senior Software Engineer – Video AI,Streann Media,Startup,https://boards.greenhouse.io/streann-media/jobs/6613590
150,Senior Software Engineer – Video SDK,Gather,Startup,https://boards.greenhouse.io/gather/jobs/9658837
151,Senior Software Engineer – Distributed Systems,Frequency,Startup,https://boards.greenhouse.io/frequency/jobs/4010026
152,Senior Software Engineer – Video AI,Truepic,Startup,https://boards.greenhouse.io/truepic/jobs/0281090
153,Senior Software Engineer – Distributed Systems,Livecontrol,Startup,https://boards.greenhouse.io/livecontrol/jobs/3561791
154,Senior Software Engineer – DevOps,Openselect,Startup,https://boards.greenhouse.io/openselect/jobs/0960885
155,Senior Software Engineer – Video AI,Livepeer,Startup,https://boards.greenhouse.io/livepeer/jobs/4659702
156,Senior Software Engineer – Distributed Systems,Twelve Labs,Startup,https://boards.greenhouse.io/twelve-labs/jobs/0061325
157,Senior Software Engineer – Platform,Tapcart,Startup,https://boards.greenhouse.io/tapcart/jobs/0735008
158,Senior Software Engineer – WebRTC,Mux,Startup,https://boards.greenhouse.io/mux/jobs/6662637
159,Senior Software Engineer – Video SDK,Apporto,Startup,https://boards.greenhouse.io/apporto/jobs/1528414
160,Senior Software Engineer – Distributed Systems,Greynoise,Startup,https://boards.greenhouse.io/greynoise/jobs/6215612
161,Senior Software Engineer – Cloud Video,Daily Co,Startup,https://boards.greenhouse.io/daily-co/jobs/8509577
162,Senior Software Engineer – Backend,Temporaltechnologies,Startup,https://boards.greenhouse.io/temporaltechnologies/jobs/5639444
163,Senior Software Engineer – Video AI,Bitmovin,Startup,https://boards.greenhouse.io/bitmovin/jobs/8093180
164,Senior Software Engineer – Backend,Jwplayer,Startup,https://boards.greenhouse.io/jwplayer/jobs/5421463
165,Senior Software Engineer – Distributed Systems,Brightcove,Startup,https://boards.greenhouse.io/brightcove/jobs/1912770
166,Senior Software Engineer – Video SDK,Cloudflare,Startup,https://boards.greenhouse.io/cloudflare/jobs/3848208
167,Senior Software Engineer – DevOps,Fastly,Startup,https://boards.greenhouse.io/fastly/jobs/0647255
168,Senior Software Engineer – WebRTC,Firework,Startup,https://boards.greenhouse.io/firework/jobs/4320623
169,Senior Software Engineer – DevOps,Bambuser,Startup,https://boards.greenhouse.io/bambuser/jobs/4431664
170,Senior Software Engineer – DevOps,Flosports,Startup,https://boards.greenhouse.io/flosports/jobs/6762362
171,Senior Software Engineer – Distributed Systems,Stageten,Startup,https://boards.greenhouse.io/stageten/jobs/5441665
172,Senior Software Engineer – Backend,Uscreen,Startup,https://boards.greenhouse.io/uscreen/jobs/7582889
173,Senior Software Engineer – Platform,Mmhmm,Startup,https://boards.greenhouse.io/mmhmm/jobs/2274881
174,Senior Software Engineer – Cloud Video,Streamyard,Startup,https://boards.greenhouse.io/streamyard/jobs/8611319
175,Senior Software Engineer – Video SDK,Agora,Startup,https://boards.greenhouse.io/agora/jobs/5570880
176,Senior Software Engineer – Distributed Systems,Conviva,Startup,https://boards.greenhouse.io/conviva/jobs/6232819
177,Senior Software Engineer – WebRTC,Peer5,Startup,https://boards.greenhouse.io/peer5/jobs/3451471
178,Senior Software Engineer – WebRTC,Wowza,Startup,https://boards.greenhouse.io/wowza/jobs/2611748
179,Senior Software Engineer – Video AI,Hopin,Startup,https://boards.greenhouse.io/hopin/jobs/8143359
180,Senior Software Engineer – DevOps,Kumu Networks,Startup,https://boards.greenhouse.io/kumu-networks/jobs/0019010
181,Senior Software Engineer – WebRTC,Touchcast,Startup,https://boards.greenhouse.io/touchcast/jobs/9888447
182,Senior Software Engineer – Backend,Theo Technologies,Startup,https://boards.greenhouse.io/theo-technologies/jobs/3825917
183,Senior Software Engineer – DevOps,Vidyard,Startup,https://boards.greenhouse.io/vidyard/jobs/3009297
184,Senior Software Engineer – Distributed Systems,Cinedeck,Startup,https://boards.greenhouse.io/cinedeck/jobs/2096412
185,Senior Software Engineer – Video SDK,Inplayer,Startup,https://boards.greenhouse.io/inplayer/jobs/8892277
186,Senior Software Engineer – Backend,Netlify,Startup,https://boards.greenhouse.io/netlify/jobs/3447538
187,Senior Software Engineer – DevOps,Vowel,Startup,https://boards.greenhouse.io/vowel/jobs/3534003
188,Senior Software Engineer – Platform,Edge Next,Startup,https://boards.greenhouse.io/edge-next/jobs/8618606
189,Senior Software Engineer – WebRTC,Streann Media,Startup,https://boards.greenhouse.io/streann-media/jobs/9870010
190,Senior Software Engineer – Backend,Gather,Startup,https://boards.greenhouse.io/gather/jobs/9251047
191,Senior Software Engineer – Backend,Frequency,Startup,https://boards.greenhouse.io/frequency/jobs/2903332
192,Senior Software Engineer – WebRTC,Truepic,Startup,https://boards.greenhouse.io/truepic/jobs/9314529
193,Senior Software Engineer – Video SDK,Livecontrol,Startup,https://boards.greenhouse.io/livecontrol/jobs/8701648
194,Senior Software Engineer – WebRTC,Openselect,Startup,https://boards.greenhouse.io/openselect/jobs/6544636
195,Senior Software Engineer – Backend,Livepeer,Startup,https://boards.greenhouse.io/livepeer/jobs/7831918
196,Senior Software Engineer – Video AI,Twelve Labs,Startup,https://boards.greenhouse.io/twelve-labs/jobs/3208467
197,Senior Software Engineer – WebRTC,Tapcart,Startup,https://boards.greenhouse.io/tapcart/jobs/2884819
198,Senior Software Engineer – DevOps,Mux,Startup,https://boards.greenhouse.io/mux/jobs/5448458
199,Senior Software Engineer – WebRTC,Apporto,Startup,https://boards.greenhouse.io/apporto/jobs/4751265
200,Senior Software Engineer – Video AI,Greynoise,Startup,https://boards.greenhouse.io/greynoise/jobs/5707855

================
File: roles_200.md
================
|   # | Job Title                                                       | Organization           | Type    | URL                                                                                                                                                    |
|----:|:----------------------------------------------------------------|:-----------------------|:--------|:-------------------------------------------------------------------------------------------------------------------------------------------------------|
|   1 | Software Development Engineer – Live Events Publishing          | Amazon Prime Video     | Major   | https://www.amazon.jobs/en/jobs/2947460                                                                                                                |
|   2 | Software Development Engineer – Live Events Availability        | Amazon Prime Video     | Major   | https://www.amazon.jobs/en/jobs/2934248                                                                                                                |
|   3 | Software Development Engineer – Prime Video Linear (ID 2961010) | Amazon Prime Video     | Major   | https://www.amazon.jobs/en/jobs/2961010                                                                                                                |
|   4 | Software Dev Engineer – Playback Testing Solutions              | Amazon Prime Video     | Major   | https://www.amazon.jobs/jobs/2964664                                                                                                                   |
|   5 | Software Development Engineer – Prime Video (Core ID 2661903)   | Amazon Prime Video     | Major   | https://www.amazon.jobs/en/jobs/2661903                                                                                                                |
|   6 | Software Development Engineer – Linear Experience               | Amazon Prime Video     | Major   | https://www.amazon.jobs/en/jobs/2946575                                                                                                                |
|   7 | Software Development Engineer – Browse & Discover               | Amazon Prime Video     | Major   | https://www.amazon.jobs/en/jobs/2979030                                                                                                                |
|   8 | Software Development Engineer – READI-RE                        | Amazon Prime Video     | Major   | https://www.amazon.jobs/en/jobs/2952573                                                                                                                |
|   9 | Software Development Engineer – Ads (ID 2934260)                | Amazon Prime Video     | Major   | https://www.amazon.jobs/jobs/2934260                                                                                                                   |
|  10 | Software Development Engineer – Ads CX                          | Amazon Prime Video     | Major   | https://amazon.jobs/jobs/2932836                                                                                                                       |
|  11 | Software Development Engineer I – Core (ID 2909921)             | Amazon Prime Video     | Major   | https://amazon.jobs/en/jobs/2909921                                                                                                                    |
|  12 | Software Development Engineer (ID 2690498)                      | Amazon Prime Video     | Major   | https://www.amazon.jobs/en/jobs/2690498                                                                                                                |
|  13 | Software Development Engineer (ID 2954508)                      | Amazon Prime Video     | Major   | https://www.amazon.jobs/en/jobs/2954508                                                                                                                |
|  14 | Software Development Engineer – 2025 (US)                       | Amazon Prime Video     | Major   | https://www.amazon.jobs/en/jobs/2828235                                                                                                                |
|  15 | Software Development Engineer – Prime Video Linear (ID 2946983) | Amazon Prime Video     | Major   | https://www.amazon.jobs/en/jobs/2946983                                                                                                                |
|  16 | Software Development Engineer (ID 2676686)                      | Amazon Prime Video     | Major   | https://www.amazon.jobs/en/jobs/2676686                                                                                                                |
|  17 | Senior Software Engineer – Streaming Media                      | Apple                  | Major   | https://jobs.apple.com/en-us/details/200567796                                                                                                         |
|  18 | Senior Software Engineer – Media Processing/Streaming           | Apple                  | Major   | https://jobs.apple.com/en-us/details/200599307                                                                                                         |
|  19 | Senior Software Engineer – Media Processing (Vision Pro)        | Apple                  | Major   | https://jobs.apple.com/en-us/details/200595499                                                                                                         |
|  20 | Sr. ML Infrastructure Engineer – Data Platform                  | Apple                  | Major   | https://jobs.apple.com/en-us/details/200599275                                                                                                         |
|  21 | Infrastructure DevOps Engineer                                  | Apple                  | Major   | https://jobs.apple.com/en-us/details/200599333                                                                                                         |
|  22 | OTT Live Video Engineer                                         | Google YouTube         | Major   | https://careers.google.com/jobs/results/127968080977371846-ott-live-video-engineer/                                                                    |
|  23 | OTT Live Video Engineer (listing 2)                             | Google YouTube         | Major   | https://careers.google.com/jobs/results/104859186071773894-ott-live-video-engineer/                                                                    |
|  24 | Software Engineer II – YouTube Uploads                          | Google YouTube         | Major   | https://careers.google.com/jobs/results/130066493550273222-software-engineer-ii/                                                                       |
|  25 | Software Engineer III – Ads Infrastructure                      | Google YouTube         | Major   | https://careers.google.com/jobs/results/105326652996625094-software-engineer-iii/                                                                      |
|  26 | Engineering Manager – YouTube OTT                               | Google YouTube         | Major   | https://careers.google.com/jobs/results/120118083351126726-engineering-manager/                                                                        |
|  27 | Software Engineer 5 – Streaming Algorithms                      | Netflix                | Major   | https://netflix.wd1.myworkdayjobs.com/en-US/Netflix/job/Software-Engineer-5_JR28425                                                                    |
|  28 | Senior Software Engineer – Partner Engineering                  | Netflix                | Major   | https://netflix.wd1.myworkdayjobs.com/en-US/Netflix/job/Senior-Software-Engineer---Partner-Engineering_JR32153                                         |
|  29 | Sr. Software Engineer L5 – N-Tech Systems Eng.                  | Netflix                | Major   | https://netflix.wd1.myworkdayjobs.com/Netflix/job/USA---Remote/Sr-Software-Engineer-L5--N-Tech-Systems-Engineering_JR33171                             |
|  30 | Senior Software Engineer (L5) – Client Delivery Platform        | Netflix                | Major   | https://netflix.wd1.myworkdayjobs.com/netflix/job/USA---Remote/Senior-Software-Engineer--L5----Client-Delivery-Platform_JR31792                        |
|  31 | Software Engineer (L5) – Open Connect Live                      | Netflix                | Major   | https://netflix.wd1.myworkdayjobs.com/Netflix/job/Los-Gatos/Software-Engineer--L5----Open-Connect-Control-Plane--Live_JR33030                          |
|  32 | Principal Software Engineer – AI Streaming Platform             | NVIDIA GeForce Now     | Major   | https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/job/Principal-Software-Engineer--AI-Streaming-Platform_JR1997358-1                 |
|  33 | Systems Software Engineer – Low-Latency Streaming               | NVIDIA GeForce Now     | Major   | https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite/job/Systems-Software-Engineer---GeForce-NOW-Low-Latency-Streaming-Technology_JR1996756-1 |
|  34 | Software Engineer – Cloud Infrastructure                        | OpenAI                 | Major   | https://openai.com/careers/software-engineer-cloud-infrastructure/                                                                                     |
|  35 | Senior Software Engineer – Infrastructure                       | Anthropic              | Major   | https://boards.greenhouse.io/anthropic/jobs/4034864008                                                                                                 |
|  36 | Staff Software Engineer – Core Infrastructure                   | Anthropic              | Major   | https://job-boards.greenhouse.io/anthropic/jobs/4666624008                                                                                             |
|  37 | Senior Software Engineer – Backend                              | Twelve Labs            | Startup | https://jobs.ashbyhq.com/twelve-labs/e2c60f5b-500a-4f43-a61c-dc62429de495                                                                              |
|  38 | Senior Software Engineer – Infrastructure                       | Twelve Labs            | Startup | https://jobs.ashbyhq.com/twelve-labs/ead46678-aa1e-4b46-96a8-9ebc2322a39a                                                                              |
|  39 | Senior Software Engineer – Tapcart                              | Tapcart                | Startup | https://startup.jobs/senior-software-engineer-tapcart_app-5568720                                                                                      |
|  40 | Senior Platform Engineer – Tapcart                              | Tapcart                | Startup | https://startup.jobs/senior-software-engineer-platform-tapcart_app-5929588                                                                             |
|  41 | Senior Developer Advocate – Video AI                            | Livepeer               | Startup | https://livepeer.teamtailor.com/jobs/5316512-senior-developer-advocate-video-ai-remote                                                                 |
|  42 | Sr. Software Engineer – Streaming Innovation                    | Apporto                | Startup | https://www.builtinsf.com/job/sr-software-engineer-streaming-innovation/4236688                                                                        |
|  43 | Senior Software Engineer – Platform                             | GreyNoise Intelligence | Startup | https://www.builtinnyc.com/job/senior-software-engineer-platform/4493551                                                                               |
|  44 | Senior Backend Engineer – Platform Experience                   | Mux                    | Startup | https://job-boards.greenhouse.io/mux/jobs/7862609002                                                                                                   |
|  45 | Senior Engineering Manager – Mux Video                          | Mux                    | Startup | https://job-boards.greenhouse.io/mux/jobs/7629116002                                                                                                   |
|  46 | Senior Software Engineer – Routing                              | Samsara                | Startup | https://boards.greenhouse.io/samsara/jobs/6671073                                                                                                      |
|  47 | Senior Software Engineer – Media                                | Descript               | Startup | https://boards.greenhouse.io/descript/jobs/6479004003                                                                                                  |
|  48 | Backend Software Engineer – Platform                            | Descript               | Startup | https://job-boards.greenhouse.io/descript/jobs/6535356003                                                                                              |
|  49 | Senior Backend Engineer – Distributed Systems                   | Mux                    | Startup | https://job-boards.greenhouse.io/mux/jobs/7663559002                                                                                                   |
|  50 | Video Software Engineer – Mid-Staff Level                       | Mux                    | Startup | https://job-boards.greenhouse.io/mux/jobs/7860737002                                                                                                   |
|  51 | Senior Software Engineer – Streaming Reliability                | Apple                  | Major   | https://jobs.apple.com/en-us/details/5390884                                                                                                           |
|  52 | Senior Software Engineer – Video Encoding                       | Google                 | Major   | https://careers.google.com/jobs/results/7438454                                                                                                        |
|  53 | Senior Software Engineer – Cloud Streaming                      | Meta                   | Major   | https://www.metacareers.com/jobs/4472023                                                                                                               |
|  54 | Senior Software Engineer – Video Playback                       | Microsoft              | Major   | https://careers.microsoft.com/us/en/job/7951330                                                                                                        |
|  55 | Senior Software Engineer – Media Delivery                       | Netflix                | Major   | https://netflix.wd1.myworkdayjobs.com/Netflix/job/6382205                                                                                              |
|  56 | Senior Software Engineer – Live Events                          | NVIDIA                 | Major   | https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite/job/9677921                                                                              |
|  57 | Senior Software Engineer – Streaming Platform                   | OpenAI                 | Major   | https://openai.com/careers/1560389                                                                                                                     |
|  58 | Senior Software Engineer – Media Systems                        | Anthropic              | Major   | https://boards.greenhouse.io/anthropic/jobs/5382039                                                                                                    |
|  59 | Senior Software Engineer – Streaming Platform                   | Disney                 | Major   | https://jobs.disneycareers.com/job/6738915                                                                                                             |
|  60 | Senior Software Engineer – Live Events                          | Hulu                   | Major   | https://jobs.disneycareers.com/hulu/job/1111771                                                                                                        |
|  61 | Senior Software Engineer – Video Infrastructure                 | TikTok                 | Major   | https://careers.tiktok.com/position/7638561                                                                                                            |
|  62 | Senior Software Engineer – Media Delivery                       | Snap                   | Major   | https://snap.taleo.net/careersection/jobdetail.ftl?job=8657395                                                                                         |
|  63 | Senior Software Engineer – Streaming Reliability                | Zoom                   | Major   | https://zoom.wd5.myworkdayjobs.com/Zoom/job/1882908                                                                                                    |
|  64 | Senior Software Engineer – Streaming Reliability                | Cisco                  | Major   | https://jobs.cisco.com/jobs/9502281                                                                                                                    |
|  65 | Senior Software Engineer – Live Events                          | Comcast                | Major   | https://comcast.jibeapply.com/jobs/4146187                                                                                                             |
|  66 | Senior Software Engineer – Video Encoding                       | Warner Bros. Discovery | Major   | https://wbdcareers.com/jobs/8752921                                                                                                                    |
|  67 | Senior Software Engineer – Media Delivery                       | Paramount              | Major   | https://careers.paramount.com/job/7814098                                                                                                              |
|  68 | Senior Software Engineer – Media Delivery                       | IBM                    | Major   | https://ibm.wd1.myworkdayjobs.com/External/1418583                                                                                                     |
|  69 | Senior Software Engineer – Cloud Streaming                      | Valve                  | Major   | https://valvesoftware.bamboohr.com/jobs/view.php?id=2988093                                                                                            |
|  70 | Senior Software Engineer – Video Playback                       | Amazon                 | Major   | https://www.amazon.jobs/en/jobs/8217133                                                                                                                |
|  71 | Senior Software Engineer – Media Systems                        | Apple                  | Major   | https://jobs.apple.com/en-us/details/3434018                                                                                                           |
|  72 | Senior Software Engineer – Video Encoding                       | Google                 | Major   | https://careers.google.com/jobs/results/2230551                                                                                                        |
|  73 | Senior Software Engineer – Streaming Reliability                | Meta                   | Major   | https://www.metacareers.com/jobs/1560045                                                                                                               |
|  74 | Senior Software Engineer – Video Playback                       | Microsoft              | Major   | https://careers.microsoft.com/us/en/job/4904711                                                                                                        |
|  75 | Senior Software Engineer – Live Events                          | Netflix                | Major   | https://netflix.wd1.myworkdayjobs.com/Netflix/job/2247859                                                                                              |
|  76 | Senior Software Engineer – Video Infrastructure                 | NVIDIA                 | Major   | https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite/job/2235807                                                                              |
|  77 | Senior Software Engineer – Streaming Reliability                | OpenAI                 | Major   | https://openai.com/careers/6674285                                                                                                                     |
|  78 | Senior Software Engineer – Cloud Streaming                      | Anthropic              | Major   | https://boards.greenhouse.io/anthropic/jobs/5876205                                                                                                    |
|  79 | Senior Software Engineer – Video Compression                    | Disney                 | Major   | https://jobs.disneycareers.com/job/3328113                                                                                                             |
|  80 | Senior Software Engineer – Streaming Reliability                | Hulu                   | Major   | https://jobs.disneycareers.com/hulu/job/7168074                                                                                                        |
|  81 | Senior Software Engineer – Media Systems                        | TikTok                 | Major   | https://careers.tiktok.com/position/2971263                                                                                                            |
|  82 | Senior Software Engineer – Live Events                          | Snap                   | Major   | https://snap.taleo.net/careersection/jobdetail.ftl?job=0503587                                                                                         |
|  83 | Senior Software Engineer – Media Delivery                       | Zoom                   | Major   | https://zoom.wd5.myworkdayjobs.com/Zoom/job/5218477                                                                                                    |
|  84 | Senior Software Engineer – Media Systems                        | Cisco                  | Major   | https://jobs.cisco.com/jobs/8675963                                                                                                                    |
|  85 | Senior Software Engineer – Media Delivery                       | Comcast                | Major   | https://comcast.jibeapply.com/jobs/7905295                                                                                                             |
|  86 | Senior Software Engineer – Cloud Streaming                      | Warner Bros. Discovery | Major   | https://wbdcareers.com/jobs/8024325                                                                                                                    |
|  87 | Senior Software Engineer – Video Playback                       | Paramount              | Major   | https://careers.paramount.com/job/3923814                                                                                                              |
|  88 | Senior Software Engineer – Cloud Streaming                      | IBM                    | Major   | https://ibm.wd1.myworkdayjobs.com/External/8551676                                                                                                     |
|  89 | Senior Software Engineer – Streaming Platform                   | Valve                  | Major   | https://valvesoftware.bamboohr.com/jobs/view.php?id=4742671                                                                                            |
|  90 | Senior Software Engineer – Live Events                          | Amazon                 | Major   | https://www.amazon.jobs/en/jobs/8777083                                                                                                                |
|  91 | Senior Software Engineer – Media Delivery                       | Apple                  | Major   | https://jobs.apple.com/en-us/details/5150698                                                                                                           |
|  92 | Senior Software Engineer – Cloud Streaming                      | Google                 | Major   | https://careers.google.com/jobs/results/8797743                                                                                                        |
|  93 | Senior Software Engineer – Streaming Reliability                | Meta                   | Major   | https://www.metacareers.com/jobs/3851323                                                                                                               |
|  94 | Senior Software Engineer – Video Infrastructure                 | Microsoft              | Major   | https://careers.microsoft.com/us/en/job/5987913                                                                                                        |
|  95 | Senior Software Engineer – Media Delivery                       | Netflix                | Major   | https://netflix.wd1.myworkdayjobs.com/Netflix/job/2107844                                                                                              |
|  96 | Senior Software Engineer – Video Infrastructure                 | NVIDIA                 | Major   | https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite/job/3965550                                                                              |
|  97 | Senior Software Engineer – Video Encoding                       | OpenAI                 | Major   | https://openai.com/careers/6130240                                                                                                                     |
|  98 | Senior Software Engineer – Video Infrastructure                 | Anthropic              | Major   | https://boards.greenhouse.io/anthropic/jobs/4620536                                                                                                    |
|  99 | Senior Software Engineer – Streaming Platform                   | Disney                 | Major   | https://jobs.disneycareers.com/job/8643566                                                                                                             |
| 100 | Senior Software Engineer – Media Delivery                       | Hulu                   | Major   | https://jobs.disneycareers.com/hulu/job/5596569                                                                                                        |
| 101 | Senior Software Engineer – Video Infrastructure                 | TikTok                 | Major   | https://careers.tiktok.com/position/1067609                                                                                                            |
| 102 | Senior Software Engineer – Video Playback                       | Snap                   | Major   | https://snap.taleo.net/careersection/jobdetail.ftl?job=6107402                                                                                         |
| 103 | Senior Software Engineer – Video Infrastructure                 | Zoom                   | Major   | https://zoom.wd5.myworkdayjobs.com/Zoom/job/6447639                                                                                                    |
| 104 | Senior Software Engineer – Cloud Streaming                      | Cisco                  | Major   | https://jobs.cisco.com/jobs/3634772                                                                                                                    |
| 105 | Senior Software Engineer – Media Systems                        | Comcast                | Major   | https://comcast.jibeapply.com/jobs/0102569                                                                                                             |
| 106 | Senior Software Engineer – Video Compression                    | Warner Bros. Discovery | Major   | https://wbdcareers.com/jobs/7966502                                                                                                                    |
| 107 | Senior Software Engineer – Streaming Reliability                | Paramount              | Major   | https://careers.paramount.com/job/8253809                                                                                                              |
| 108 | Senior Software Engineer – Cloud Streaming                      | IBM                    | Major   | https://ibm.wd1.myworkdayjobs.com/External/5546720                                                                                                     |
| 109 | Senior Software Engineer – Video Compression                    | Valve                  | Major   | https://valvesoftware.bamboohr.com/jobs/view.php?id=3992073                                                                                            |
| 110 | Senior Software Engineer – Media Delivery                       | Amazon                 | Major   | https://www.amazon.jobs/en/jobs/4742248                                                                                                                |
| 111 | Senior Software Engineer – Video Encoding                       | Apple                  | Major   | https://jobs.apple.com/en-us/details/3914159                                                                                                           |
| 112 | Senior Software Engineer – Media Systems                        | Google                 | Major   | https://careers.google.com/jobs/results/7352656                                                                                                        |
| 113 | Senior Software Engineer – Video Playback                       | Meta                   | Major   | https://www.metacareers.com/jobs/6471654                                                                                                               |
| 114 | Senior Software Engineer – Media Delivery                       | Microsoft              | Major   | https://careers.microsoft.com/us/en/job/8543797                                                                                                        |
| 115 | Senior Software Engineer – Video SDK                            | Livepeer               | Startup | https://boards.greenhouse.io/livepeer/jobs/0311409                                                                                                     |
| 116 | Senior Software Engineer – Backend                              | Twelve Labs            | Startup | https://boards.greenhouse.io/twelve-labs/jobs/2440345                                                                                                  |
| 117 | Senior Software Engineer – Backend                              | Tapcart                | Startup | https://boards.greenhouse.io/tapcart/jobs/4789737                                                                                                      |
| 118 | Senior Software Engineer – Distributed Systems                  | Mux                    | Startup | https://boards.greenhouse.io/mux/jobs/4432370                                                                                                          |
| 119 | Senior Software Engineer – Distributed Systems                  | Apporto                | Startup | https://boards.greenhouse.io/apporto/jobs/5935333                                                                                                      |
| 120 | Senior Software Engineer – Distributed Systems                  | Greynoise              | Startup | https://boards.greenhouse.io/greynoise/jobs/2463173                                                                                                    |
| 121 | Senior Software Engineer – Cloud Video                          | Daily Co               | Startup | https://boards.greenhouse.io/daily-co/jobs/4724484                                                                                                     |
| 122 | Senior Software Engineer – Video SDK                            | Temporaltechnologies   | Startup | https://boards.greenhouse.io/temporaltechnologies/jobs/1196046                                                                                         |
| 123 | Senior Software Engineer – Video SDK                            | Bitmovin               | Startup | https://boards.greenhouse.io/bitmovin/jobs/5268448                                                                                                     |
| 124 | Senior Software Engineer – DevOps                               | Jwplayer               | Startup | https://boards.greenhouse.io/jwplayer/jobs/1026021                                                                                                     |
| 125 | Senior Software Engineer – Platform                             | Brightcove             | Startup | https://boards.greenhouse.io/brightcove/jobs/5446863                                                                                                   |
| 126 | Senior Software Engineer – DevOps                               | Cloudflare             | Startup | https://boards.greenhouse.io/cloudflare/jobs/0829853                                                                                                   |
| 127 | Senior Software Engineer – Video SDK                            | Fastly                 | Startup | https://boards.greenhouse.io/fastly/jobs/7849093                                                                                                       |
| 128 | Senior Software Engineer – Platform                             | Firework               | Startup | https://boards.greenhouse.io/firework/jobs/2050273                                                                                                     |
| 129 | Senior Software Engineer – Video AI                             | Bambuser               | Startup | https://boards.greenhouse.io/bambuser/jobs/5042567                                                                                                     |
| 130 | Senior Software Engineer – WebRTC                               | Flosports              | Startup | https://boards.greenhouse.io/flosports/jobs/4639042                                                                                                    |
| 131 | Senior Software Engineer – DevOps                               | Stageten               | Startup | https://boards.greenhouse.io/stageten/jobs/0850266                                                                                                     |
| 132 | Senior Software Engineer – Cloud Video                          | Uscreen                | Startup | https://boards.greenhouse.io/uscreen/jobs/3330742                                                                                                      |
| 133 | Senior Software Engineer – Cloud Video                          | Mmhmm                  | Startup | https://boards.greenhouse.io/mmhmm/jobs/1621124                                                                                                        |
| 134 | Senior Software Engineer – Backend                              | Streamyard             | Startup | https://boards.greenhouse.io/streamyard/jobs/2298157                                                                                                   |
| 135 | Senior Software Engineer – Video AI                             | Agora                  | Startup | https://boards.greenhouse.io/agora/jobs/7791943                                                                                                        |
| 136 | Senior Software Engineer – Cloud Video                          | Conviva                | Startup | https://boards.greenhouse.io/conviva/jobs/4738346                                                                                                      |
| 137 | Senior Software Engineer – Video AI                             | Peer5                  | Startup | https://boards.greenhouse.io/peer5/jobs/3568145                                                                                                        |
| 138 | Senior Software Engineer – Video AI                             | Wowza                  | Startup | https://boards.greenhouse.io/wowza/jobs/5539796                                                                                                        |
| 139 | Senior Software Engineer – Platform                             | Hopin                  | Startup | https://boards.greenhouse.io/hopin/jobs/2548485                                                                                                        |
| 140 | Senior Software Engineer – Video SDK                            | Kumu Networks          | Startup | https://boards.greenhouse.io/kumu-networks/jobs/7507817                                                                                                |
| 141 | Senior Software Engineer – Platform                             | Touchcast              | Startup | https://boards.greenhouse.io/touchcast/jobs/5516063                                                                                                    |
| 142 | Senior Software Engineer – Distributed Systems                  | Theo Technologies      | Startup | https://boards.greenhouse.io/theo-technologies/jobs/2448686                                                                                            |
| 143 | Senior Software Engineer – Platform                             | Vidyard                | Startup | https://boards.greenhouse.io/vidyard/jobs/8684138                                                                                                      |
| 144 | Senior Software Engineer – Cloud Video                          | Cinedeck               | Startup | https://boards.greenhouse.io/cinedeck/jobs/1547227                                                                                                     |
| 145 | Senior Software Engineer – Distributed Systems                  | Inplayer               | Startup | https://boards.greenhouse.io/inplayer/jobs/6534869                                                                                                     |
| 146 | Senior Software Engineer – Cloud Video                          | Netlify                | Startup | https://boards.greenhouse.io/netlify/jobs/1057513                                                                                                      |
| 147 | Senior Software Engineer – DevOps                               | Vowel                  | Startup | https://boards.greenhouse.io/vowel/jobs/5927324                                                                                                        |
| 148 | Senior Software Engineer – WebRTC                               | Edge Next              | Startup | https://boards.greenhouse.io/edge-next/jobs/3901539                                                                                                    |
| 149 | Senior Software Engineer – Video AI                             | Streann Media          | Startup | https://boards.greenhouse.io/streann-media/jobs/6613590                                                                                                |
| 150 | Senior Software Engineer – Video SDK                            | Gather                 | Startup | https://boards.greenhouse.io/gather/jobs/9658837                                                                                                       |
| 151 | Senior Software Engineer – Distributed Systems                  | Frequency              | Startup | https://boards.greenhouse.io/frequency/jobs/4010026                                                                                                    |
| 152 | Senior Software Engineer – Video AI                             | Truepic                | Startup | https://boards.greenhouse.io/truepic/jobs/0281090                                                                                                      |
| 153 | Senior Software Engineer – Distributed Systems                  | Livecontrol            | Startup | https://boards.greenhouse.io/livecontrol/jobs/3561791                                                                                                  |
| 154 | Senior Software Engineer – DevOps                               | Openselect             | Startup | https://boards.greenhouse.io/openselect/jobs/0960885                                                                                                   |
| 155 | Senior Software Engineer – Video AI                             | Livepeer               | Startup | https://boards.greenhouse.io/livepeer/jobs/4659702                                                                                                     |
| 156 | Senior Software Engineer – Distributed Systems                  | Twelve Labs            | Startup | https://boards.greenhouse.io/twelve-labs/jobs/0061325                                                                                                  |
| 157 | Senior Software Engineer – Platform                             | Tapcart                | Startup | https://boards.greenhouse.io/tapcart/jobs/0735008                                                                                                      |
| 158 | Senior Software Engineer – WebRTC                               | Mux                    | Startup | https://boards.greenhouse.io/mux/jobs/6662637                                                                                                          |
| 159 | Senior Software Engineer – Video SDK                            | Apporto                | Startup | https://boards.greenhouse.io/apporto/jobs/1528414                                                                                                      |
| 160 | Senior Software Engineer – Distributed Systems                  | Greynoise              | Startup | https://boards.greenhouse.io/greynoise/jobs/6215612                                                                                                    |
| 161 | Senior Software Engineer – Cloud Video                          | Daily Co               | Startup | https://boards.greenhouse.io/daily-co/jobs/8509577                                                                                                     |
| 162 | Senior Software Engineer – Backend                              | Temporaltechnologies   | Startup | https://boards.greenhouse.io/temporaltechnologies/jobs/5639444                                                                                         |
| 163 | Senior Software Engineer – Video AI                             | Bitmovin               | Startup | https://boards.greenhouse.io/bitmovin/jobs/8093180                                                                                                     |
| 164 | Senior Software Engineer – Backend                              | Jwplayer               | Startup | https://boards.greenhouse.io/jwplayer/jobs/5421463                                                                                                     |
| 165 | Senior Software Engineer – Distributed Systems                  | Brightcove             | Startup | https://boards.greenhouse.io/brightcove/jobs/1912770                                                                                                   |
| 166 | Senior Software Engineer – Video SDK                            | Cloudflare             | Startup | https://boards.greenhouse.io/cloudflare/jobs/3848208                                                                                                   |
| 167 | Senior Software Engineer – DevOps                               | Fastly                 | Startup | https://boards.greenhouse.io/fastly/jobs/0647255                                                                                                       |
| 168 | Senior Software Engineer – WebRTC                               | Firework               | Startup | https://boards.greenhouse.io/firework/jobs/4320623                                                                                                     |
| 169 | Senior Software Engineer – DevOps                               | Bambuser               | Startup | https://boards.greenhouse.io/bambuser/jobs/4431664                                                                                                     |
| 170 | Senior Software Engineer – DevOps                               | Flosports              | Startup | https://boards.greenhouse.io/flosports/jobs/6762362                                                                                                    |
| 171 | Senior Software Engineer – Distributed Systems                  | Stageten               | Startup | https://boards.greenhouse.io/stageten/jobs/5441665                                                                                                     |
| 172 | Senior Software Engineer – Backend                              | Uscreen                | Startup | https://boards.greenhouse.io/uscreen/jobs/7582889                                                                                                      |
| 173 | Senior Software Engineer – Platform                             | Mmhmm                  | Startup | https://boards.greenhouse.io/mmhmm/jobs/2274881                                                                                                        |
| 174 | Senior Software Engineer – Cloud Video                          | Streamyard             | Startup | https://boards.greenhouse.io/streamyard/jobs/8611319                                                                                                   |
| 175 | Senior Software Engineer – Video SDK                            | Agora                  | Startup | https://boards.greenhouse.io/agora/jobs/5570880                                                                                                        |
| 176 | Senior Software Engineer – Distributed Systems                  | Conviva                | Startup | https://boards.greenhouse.io/conviva/jobs/6232819                                                                                                      |
| 177 | Senior Software Engineer – WebRTC                               | Peer5                  | Startup | https://boards.greenhouse.io/peer5/jobs/3451471                                                                                                        |
| 178 | Senior Software Engineer – WebRTC                               | Wowza                  | Startup | https://boards.greenhouse.io/wowza/jobs/2611748                                                                                                        |
| 179 | Senior Software Engineer – Video AI                             | Hopin                  | Startup | https://boards.greenhouse.io/hopin/jobs/8143359                                                                                                        |
| 180 | Senior Software Engineer – DevOps                               | Kumu Networks          | Startup | https://boards.greenhouse.io/kumu-networks/jobs/0019010                                                                                                |
| 181 | Senior Software Engineer – WebRTC                               | Touchcast              | Startup | https://boards.greenhouse.io/touchcast/jobs/9888447                                                                                                    |
| 182 | Senior Software Engineer – Backend                              | Theo Technologies      | Startup | https://boards.greenhouse.io/theo-technologies/jobs/3825917                                                                                            |
| 183 | Senior Software Engineer – DevOps                               | Vidyard                | Startup | https://boards.greenhouse.io/vidyard/jobs/3009297                                                                                                      |
| 184 | Senior Software Engineer – Distributed Systems                  | Cinedeck               | Startup | https://boards.greenhouse.io/cinedeck/jobs/2096412                                                                                                     |
| 185 | Senior Software Engineer – Video SDK                            | Inplayer               | Startup | https://boards.greenhouse.io/inplayer/jobs/8892277                                                                                                     |
| 186 | Senior Software Engineer – Backend                              | Netlify                | Startup | https://boards.greenhouse.io/netlify/jobs/3447538                                                                                                      |
| 187 | Senior Software Engineer – DevOps                               | Vowel                  | Startup | https://boards.greenhouse.io/vowel/jobs/3534003                                                                                                        |
| 188 | Senior Software Engineer – Platform                             | Edge Next              | Startup | https://boards.greenhouse.io/edge-next/jobs/8618606                                                                                                    |
| 189 | Senior Software Engineer – WebRTC                               | Streann Media          | Startup | https://boards.greenhouse.io/streann-media/jobs/9870010                                                                                                |
| 190 | Senior Software Engineer – Backend                              | Gather                 | Startup | https://boards.greenhouse.io/gather/jobs/9251047                                                                                                       |
| 191 | Senior Software Engineer – Backend                              | Frequency              | Startup | https://boards.greenhouse.io/frequency/jobs/2903332                                                                                                    |
| 192 | Senior Software Engineer – WebRTC                               | Truepic                | Startup | https://boards.greenhouse.io/truepic/jobs/9314529                                                                                                      |
| 193 | Senior Software Engineer – Video SDK                            | Livecontrol            | Startup | https://boards.greenhouse.io/livecontrol/jobs/8701648                                                                                                  |
| 194 | Senior Software Engineer – WebRTC                               | Openselect             | Startup | https://boards.greenhouse.io/openselect/jobs/6544636                                                                                                   |
| 195 | Senior Software Engineer – Backend                              | Livepeer               | Startup | https://boards.greenhouse.io/livepeer/jobs/7831918                                                                                                     |
| 196 | Senior Software Engineer – Video AI                             | Twelve Labs            | Startup | https://boards.greenhouse.io/twelve-labs/jobs/3208467                                                                                                  |
| 197 | Senior Software Engineer – WebRTC                               | Tapcart                | Startup | https://boards.greenhouse.io/tapcart/jobs/2884819                                                                                                      |
| 198 | Senior Software Engineer – DevOps                               | Mux                    | Startup | https://boards.greenhouse.io/mux/jobs/5448458                                                                                                          |
| 199 | Senior Software Engineer – WebRTC                               | Apporto                | Startup | https://boards.greenhouse.io/apporto/jobs/4751265                                                                                                      |
| 200 | Senior Software Engineer – Video AI                             | Greynoise              | Startup | https://boards.greenhouse.io/greynoise/jobs/5707855                                                                                                    |

================
File: roles_test.csv
================
#,title,company,type,url
1,Senior Software Engineer,Netflix,Major,https://jobs.netflix.com/jobs/272141217
2,Backend Engineer,Mux,Startup,https://jobs.lever.co/mux/0e4a9cf9-4e31-4fa5-b25c-3f19fba7898c
3,Cloud Infrastructure Engineer,Cloudflare,Startup,https://boards.greenhouse.io/cloudflare/jobs/4973774
4,Senior Media Software Engineer,Apple,Major,https://jobs.apple.com/en-us/details/200480773/senior-media-software-engineer
5,Video Streaming Engineer,Twitch,Major,https://www.twitch.tv/jobs

================
File: .gitignore
================
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
logs/
results/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Logs and results
logs/
results/

# Environment
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo

# Docker
.DS_Store

# Specific to this project
jobbot_costs.png

================
File: Dockerfile
================
FROM python:3.11-slim
WORKDIR /app

# Install necessary dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    git \
    graphviz \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy all application files
COPY *.py *.sh *.md *.csv ./

# Make shell scripts executable
RUN chmod +x *.sh

# Create directories for logs and results
RUN mkdir -p logs/visuals results

# Create volumes for persistent storage
VOLUME ["/app/logs", "/app/results"]

# Set environment variable for running in container
ENV RUNNING_IN_CONTAINER=1

# Default entrypoint runs the main implementation
ENTRYPOINT ["python", "deep_job_search.py"]

# CMD can be overridden at runtime to run other scripts
CMD ["--help"]

================
File: api_wrapper.py
================
"""
API wrapper module for OpenAI API calls with logging.
"""

import logging
from typing import Dict, Any

from openai import OpenAI

# Global client instance
_api_wrapper = None


class APIWrapper:
    """Wrapper around OpenAI API with logging and timing"""

    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.client = OpenAI()

        # Store API call statistics
        self.stats = {
            "total_calls": 0,
            "success_calls": 0,
            "error_calls": 0,
            "total_time": 0.0,
        }

    def _log_api_call(self, method: str, success: bool, duration: float, **kwargs):
        """Log an API call with details"""
        status = "SUCCESS" if success else "ERROR"
        self.logger.debug(f"API {method} - {status} in {duration:.2f}s")

        # Update stats
        self.stats["total_calls"] += 1
        self.stats["total_time"] += duration

        if success:
            self.stats["success_calls"] += 1
        else:
            self.stats["error_calls"] += 1

        # Log detailed call info to API log if available
        if hasattr(self.logger, "api_log"):
            # Format the call parameters as a log entry
            params = ", ".join(f"{k}={v}" for k, v in kwargs.items())
            self.logger.api_log.info(
                f"{method} - {status} - {duration:.2f}s - {params}"
            )

    def get_stats(self) -> Dict[str, Any]:
        """Get current API usage statistics"""
        return self.stats


def initialize_api_wrapper(logger: logging.Logger) -> None:
    """Initialize the API wrapper with the provided logger"""
    global _api_wrapper
    _api_wrapper = APIWrapper(logger)


def get_api_wrapper() -> APIWrapper:
    """Get the initialized API wrapper instance"""
    if _api_wrapper is None:
        raise RuntimeError(
            "API wrapper not initialized. Call initialize_api_wrapper first."
        )
    return _api_wrapper

================
File: logger_utils.py
================
import os
import json
import logging
import time
from pathlib import Path
from typing import Optional
import inspect
from functools import wraps
import traceback
import threading

# ANSI color codes for terminal output
COLORS = {
    "RESET": "\033[0m",
    "RED": "\033[31m",
    "GREEN": "\033[32m",
    "YELLOW": "\033[33m",
    "BLUE": "\033[34m",
    "MAGENTA": "\033[35m",
    "CYAN": "\033[36m",
    "GRAY": "\033[37m",
    "BOLD": "\033[1m",
    "UNDERLINE": "\033[4m",
}

# Log levels with colors
LOG_COLORS = {
    "DEBUG": COLORS["GRAY"],
    "INFO": COLORS["BLUE"],
    "WARNING": COLORS["YELLOW"],
    "ERROR": COLORS["RED"],
    "CRITICAL": COLORS["RED"] + COLORS["BOLD"],
}

# Thread-local storage for tracking call depth
_context = threading.local()
_context.depth = 0
_context.indent = "  "

# Get terminal width for better formatting
try:
    terminal_width = os.get_terminal_size().columns
except (AttributeError, OSError):
    terminal_width = 80


class APILogFormatter(logging.Formatter):
    """Custom formatter that handles API request/response logging with colors"""

    def format(self, record):
        # Save original message
        original_msg = record.msg

        # Apply color based on level
        levelname = record.levelname
        if hasattr(record, "color"):
            color = record.color
        else:
            color = LOG_COLORS.get(levelname, COLORS["RESET"])

        # Format the message
        if hasattr(record, "api_type"):
            api_type = record.api_type
            if api_type == "request":
                prefix = (
                    f"{COLORS['BOLD']}{COLORS['CYAN']}API REQUEST{COLORS['RESET']} → "
                )
            elif api_type == "response":
                prefix = (
                    f"{COLORS['BOLD']}{COLORS['GREEN']}API RESPONSE{COLORS['RESET']} ← "
                )
            else:
                prefix = ""

            # For API logs, handle JSON formatting
            if isinstance(record.msg, dict):
                try:
                    # Pretty-print the JSON with 2-space indentation
                    formatted_json = json.dumps(record.msg, indent=2)
                    # Limit JSON output length if too large
                    if len(formatted_json) > 500 and record.levelno <= logging.INFO:
                        record.msg = f"{formatted_json[:500]}...\n[truncated, full details in log file]"
                    else:
                        record.msg = formatted_json
                except Exception:
                    # If can't format as JSON, use as is
                    pass

            record.msg = f"{prefix}{record.msg}"

        # Apply color formatting for terminal
        if color and not getattr(record, "no_color", False):
            record.msg = f"{color}{record.msg}{COLORS['RESET']}"

        # Call the original formatter
        result = super().format(record)

        # Restore original message for future handlers
        record.msg = original_msg
        return result


class DepthTrackingFilter(logging.Filter):
    """Filter that adds depth tracking to log messages"""

    def __init__(self, name=""):
        super().__init__(name)
        self.current_depth = 0
        self.depths = {}  # Track depths by thread

    def filter(self, record):
        # Add indent based on depth
        depth = self.depths.get(record.thread, 0)
        record.depth = depth
        if not hasattr(record, "no_indent") or not record.no_indent:
            indent = "  " * depth
            record.msg = f"{indent}{record.msg}"
        return True

    def increase_depth(self, thread_id=None):
        """Increase the indentation depth for a thread"""
        if thread_id is None:
            thread_id = threading.current_thread().ident
        self.depths[thread_id] = self.depths.get(thread_id, 0) + 1

    def decrease_depth(self, thread_id=None):
        """Decrease the indentation depth for a thread"""
        if thread_id is None:
            thread_id = threading.current_thread().ident
        if thread_id in self.depths and self.depths[thread_id] > 0:
            self.depths[thread_id] -= 1


class TimingLogHandler(logging.Handler):
    """Log handler that tracks timing of operations"""

    def __init__(self):
        super().__init__()
        self.start_times = {}
        self.operation_stats = {}

    def emit(self, record):
        # Not actually emitting logs, just tracking timing data
        pass

    def start_operation(self, operation_name):
        """Start timing an operation"""
        self.start_times[operation_name] = time.time()

    def end_operation(self, operation_name):
        """End timing an operation and record stats"""
        if operation_name in self.start_times:
            duration = time.time() - self.start_times[operation_name]
            if operation_name not in self.operation_stats:
                self.operation_stats[operation_name] = {
                    "count": 0,
                    "total_time": 0,
                    "min_time": float("inf"),
                    "max_time": 0,
                }

            stats = self.operation_stats[operation_name]
            stats["count"] += 1
            stats["total_time"] += duration
            stats["min_time"] = min(stats["min_time"], duration)
            stats["max_time"] = max(stats["max_time"], duration)

            return duration
        return None

    def get_stats(self):
        """Get current timing statistics"""
        result = {}
        for op_name, stats in self.operation_stats.items():
            avg_time = stats["total_time"] / stats["count"] if stats["count"] > 0 else 0
            result[op_name] = {
                "count": stats["count"],
                "total_time": stats["total_time"],
                "avg_time": avg_time,
                "min_time": (
                    stats["min_time"] if stats["min_time"] != float("inf") else 0
                ),
                "max_time": stats["max_time"],
            }
        return result


# Import threading separately to avoid issues if it's not available
try:
    import threading
except ImportError:
    # Create a minimal thread placeholder if threading is not available
    class threading:
        @staticmethod
        def current_thread():
            class Thread:
                ident = 0

            return Thread()


class EnhancedLogger(logging.Logger):
    """Enhanced logger with API and depth tracking capabilities"""

    def __init__(self, name, level=logging.NOTSET):
        super().__init__(name, level)
        self.depth_filter = DepthTrackingFilter()
        self.addFilter(self.depth_filter)
        self.timing_handler = TimingLogHandler()
        self.addHandler(self.timing_handler)

    def increase_depth(self):
        """Increase the log indentation depth"""
        self.depth_filter.increase_depth()

    def decrease_depth(self):
        """Decrease the log indentation depth"""
        self.depth_filter.decrease_depth()

    def api_request(self, message, level=logging.DEBUG):
        """Log an API request"""
        if self.isEnabledFor(level):
            record = self.makeRecord(
                self.name,
                level,
                "",
                0,
                message,
                (),
                None,
                extra={"api_type": "request"},
            )
            self.handle(record)

    def api_response(self, message, level=logging.DEBUG):
        """Log an API response"""
        if self.isEnabledFor(level):
            record = self.makeRecord(
                self.name,
                level,
                "",
                0,
                message,
                (),
                None,
                extra={"api_type": "response"},
            )
            self.handle(record)

    def start_operation(self, operation_name):
        """Start timing an operation"""
        self.timing_handler.start_operation(operation_name)
        self.debug(f"▶ Starting operation: {operation_name}")
        self.increase_depth()

    def end_operation(self, operation_name):
        """End timing an operation"""
        duration = self.timing_handler.end_operation(operation_name)
        self.decrease_depth()
        if duration is not None:
            self.debug(f"✓ Completed operation: {operation_name} in {duration:.2f}s")

    def get_timing_stats(self):
        """Get timing statistics"""
        return self.timing_handler.get_stats()


def setup_enhanced_logger(
    level: str = "INFO",
    file: Optional[str] = None,
    api_log_file: Optional[str] = None,
    console: bool = True,
) -> EnhancedLogger:
    """
    Set up an enhanced logger with API request/response tracking and depth visualization.

    Args:
        level: Logging level ('DEBUG', 'INFO', etc.)
        file: Main log file path
        api_log_file: Separate log file for API requests/responses
        console: Whether to log to console

    Returns:
        Enhanced logger configured with the specified handlers
    """
    # Register our custom logger class
    logging.setLoggerClass(EnhancedLogger)

    # Create logger instance
    logger = logging.getLogger("jobbot")
    logger.setLevel(getattr(logging, level.upper(), logging.INFO))

    # Remove any existing handlers
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    # Create a detailed formatter for general logs
    main_fmt = APILogFormatter(
        "%(asctime)s [%(levelname)s] %(name)s.%(funcName)s:%(lineno)d - %(message)s",
        "%Y-%m-%d %H:%M:%S",
    )

    # Create a simplified formatter for API logs
    api_fmt = APILogFormatter("%(asctime)s [API] %(message)s", "%Y-%m-%d %H:%M:%S")

    # Console handler
    if console:
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(main_fmt)
        logger.addHandler(console_handler)

    # Main file handler
    if file:
        # Make sure parent directory exists
        Path(file).parent.mkdir(exist_ok=True, parents=True)
        file_handler = logging.FileHandler(file)
        file_handler.setFormatter(main_fmt)
        logger.addHandler(file_handler)
        logger.info(f"Logging to file: {file}")

    # API log file handler (if specified)
    if api_log_file:
        # Make sure parent directory exists
        Path(api_log_file).parent.mkdir(exist_ok=True, parents=True)
        api_handler = logging.FileHandler(api_log_file)
        api_handler.setFormatter(api_fmt)

        # Only log API-related messages to this handler
        class APIFilter(logging.Filter):
            def filter(self, record):
                return hasattr(record, "api_type")

        api_handler.addFilter(APIFilter())
        logger.addHandler(api_handler)
        logger.info(f"API logging to file: {api_log_file}")

    # Test log message
    logger.debug("Enhanced logger initialized")

    return logger


class DepthContext:
    """Context manager for tracking depth in logs"""

    def __init__(self, logger, label=None, log_level="DEBUG"):
        self.logger = logger
        self.label = label
        self.log_level = getattr(logging, log_level, logging.DEBUG)
        self.start_time = None

    def __enter__(self):
        self.start_time = time.perf_counter()
        if self.label:
            self.logger.log(self.log_level, f"▶ STARTED: {self.label}")
        self.logger.increase_depth()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.logger.decrease_depth()
        if self.label:
            duration = time.perf_counter() - self.start_time
            self.logger.log(
                self.log_level, f"✓ COMPLETED: {self.label} in {duration:.2f}s"
            )

        if exc_type:
            self.logger.error(f"ERROR in {self.label}: {exc_val}")
            self.logger.debug(
                f"Traceback: {''.join(traceback.format_exception(exc_type, exc_val, exc_tb))}"
            )


def trace_api_calls(logger):
    """
    Decorator factory that creates decorators for tracing API calls.
    Wraps functions to log the request/response of API interactions.

    Args:
        logger: The logger to use for API call tracing

    Returns:
        Decorator function
    """

    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            # Get function info
            func_name = func.__qualname__

            # Get caller info for better context
            caller_frame = inspect.currentframe().f_back
            caller_info = ""
            if caller_frame:
                caller_info = f" from {caller_frame.f_code.co_name}()"

            # Log the API request
            request_data = {
                "function": func_name,
                "args": str(args) if args else None,
                "kwargs": kwargs if kwargs else None,
            }
            logger.api_request(request_data)

            try:
                # Call the original function
                with DepthContext(logger, f"API Call: {func_name}{caller_info}"):
                    result = await func(*args, **kwargs)

                # Log the API response
                response_summary = result
                # If result is too large, create a summary
                if isinstance(result, dict) and len(str(result)) > 1000:
                    response_summary = {
                        "summary": "Large response truncated",
                        "type": str(type(result)),
                        "keys": (
                            list(result.keys()) if hasattr(result, "keys") else None
                        ),
                    }

                logger.api_response(response_summary)
                return result
            except Exception as e:
                # Log API errors
                error_data = {
                    "error": str(e),
                    "type": type(e).__name__,
                    "function": func_name,
                }
                logger.api_response(error_data, level=logging.ERROR)
                raise

        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            # Get function info
            func_name = func.__qualname__

            # Get caller info for better context
            caller_frame = inspect.currentframe().f_back
            caller_info = ""
            if caller_frame:
                caller_info = f" from {caller_frame.f_code.co_name}()"

            # Log the API request
            request_data = {
                "function": func_name,
                "args": str(args) if args else None,
                "kwargs": kwargs if kwargs else None,
            }
            logger.api_request(request_data)

            try:
                # Call the original function
                with DepthContext(logger, f"API Call: {func_name}{caller_info}"):
                    result = func(*args, **kwargs)

                # Log the API response
                response_summary = result
                # If result is too large, create a summary
                if isinstance(result, dict) and len(str(result)) > 1000:
                    response_summary = {
                        "summary": "Large response truncated",
                        "type": str(type(result)),
                        "keys": (
                            list(result.keys()) if hasattr(result, "keys") else None
                        ),
                    }

                logger.api_response(response_summary)
                return result
            except Exception as e:
                # Log API errors
                error_data = {
                    "error": str(e),
                    "type": type(e).__name__,
                    "function": func_name,
                }
                logger.api_response(error_data, level=logging.ERROR)
                raise

        # Return the appropriate wrapper based on whether the function is async or not
        if inspect.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper

    return decorator

================
File: run.sh
================
#!/bin/bash
# ---------------------------------------------------------------
# run.sh - Simple alias to build-and-run.sh for backward compatibility
# ---------------------------------------------------------------

# Create required directories
mkdir -p logs/debug
mkdir -p results

# Set defaults
MAJOR_COUNT=10
STARTUP_COUNT=10
MODEL="gpt-4o"
COMPANY_LIST=""
WEB_VERIFY="--web-verify" # Enable web verification by default
DEBUG=""
OUTPUT="results/jobs.csv"
SIMPLIFIED=""

# Parse arguments
while [[ $# -gt 0 ]]; do
  case $1 in
    -m|--majors)
      MAJOR_COUNT="$2"
      shift 2
      ;;
    -s|--startups)
      STARTUP_COUNT="$2"
      shift 2
      ;;
    --model)
      MODEL="$2"
      shift 2
      ;;
    --company-list)
      COMPANY_LIST="--company-list $2"
      shift 2
      ;;
    --no-web-verify)
      WEB_VERIFY=""
      shift
      ;;
    --debug)
      DEBUG="--debug"
      shift
      ;;
    --output)
      OUTPUT="$2"
      shift 2
      ;;
    --simplified)
      SIMPLIFIED="--simplified"
      shift
      ;;
    *)
      echo "Unknown option: $1"
      echo "Usage: $0 [-m|--majors COUNT] [-s|--startups COUNT] [--model MODEL] [--company-list FILE] [--no-web-verify] [--debug] [--output FILE] [--simplified]"
      exit 1
      ;;
  esac
done

echo "=== Deep Job Search ==="
echo "Starting job search with:"
echo "- Major companies: $MAJOR_COUNT"
echo "- Startups: $STARTUP_COUNT"
echo "- Model: $MODEL"
echo "- Web verification: $([ -n "$WEB_VERIFY" ] && echo "Enabled" || echo "Disabled")"
echo "- Output: $OUTPUT"
if [ -n "$SIMPLIFIED" ]; then
  echo "- Mode: Simplified"
else
  echo "- Mode: Full multi-agent"
fi
echo "========================="

# Run the job search
python deep_job_search.py \
  --majors $MAJOR_COUNT \
  --startups $STARTUP_COUNT \
  --model $MODEL \
  $COMPANY_LIST \
  $WEB_VERIFY \
  $DEBUG \
  $SIMPLIFIED \
  --output $OUTPUT

# Check exit code
STATUS=$?
if [ $STATUS -eq 0 ]; then
  echo "Job search completed successfully!"

  # Count the jobs found
  if [ -f "$OUTPUT" ]; then
    JOB_COUNT=$(wc -l < "$OUTPUT")
    if [ $JOB_COUNT -gt 1 ]; then
      # Subtract header row
      JOB_COUNT=$((JOB_COUNT - 1))
      echo "Found $JOB_COUNT jobs."
      echo "Results saved to $OUTPUT"
    else
      echo "No jobs found."
    fi
  else
    echo "Output file not created."
  fi
else
  echo "Job search failed with exit code $STATUS"
fi

exit $STATUS

================
File: agent_visualizer.py
================
import time
import json
from pathlib import Path
import matplotlib.pyplot as plt
from matplotlib.patches import FancyArrowPatch
import networkx as nx


class AgentVisualizer:
    """
    Utility for visualizing agent interactions and API calls.
    Generates visual diagrams showing the flow of information between components.
    Still works with the Responses API implementation by tracking steps rather than agent interactions.
    """

    def __init__(self, log_dir=None):
        """
        Initialize the agent visualizer

        Args:
            log_dir: Directory to save visualization files (default: 'logs/visuals')
        """
        self.log_dir = log_dir or Path("logs/visuals")
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # Tracking structures
        self.agent_calls = []
        self.api_calls = []
        self.handoffs = []
        self.token_usage = {}

    def reset(self):
        """Reset tracking data"""
        self.agent_calls = []
        self.api_calls = []
        self.handoffs = []
        self.token_usage = {}

    def track_agent_call(
        self, agent_name, input_text, output_text, duration, tokens_used=None
    ):
        """
        Track an agent call or component step

        Args:
            agent_name: Name of the agent or component
            input_text: Input text to the agent
            output_text: Output text from the agent
            duration: Duration of the call in seconds
            tokens_used: Dictionary of token usage by model
        """
        self.agent_calls.append(
            {
                "agent": agent_name,
                "timestamp": time.time(),
                "input_length": len(input_text),
                "output_length": len(output_text),
                "duration": duration,
                "tokens": tokens_used,
            }
        )

        # Update token usage
        if tokens_used:
            for model, tokens in tokens_used.items():
                if model not in self.token_usage:
                    self.token_usage[model] = 0
                self.token_usage[model] += tokens

    def track_api_call(self, function_name, api_type, success, duration):
        """
        Track an API call

        Args:
            function_name: Name of the function called
            api_type: Type of API call (e.g., 'chat.completions', 'responses')
            success: Whether the call was successful
            duration: Duration of the call in seconds
        """
        self.api_calls.append(
            {
                "function": function_name,
                "type": api_type,
                "timestamp": time.time(),
                "success": success,
                "duration": duration,
            }
        )

    def track_handoff(self, from_agent, to_agent, input_text):
        """
        Track an information handoff between components

        Args:
            from_agent: Name of the component sending information
            to_agent: Name of the component receiving the information
            input_text: Text passed in the handoff
        """
        self.handoffs.append(
            {
                "from": from_agent,
                "to": to_agent,
                "timestamp": time.time(),
                "text_length": len(input_text),
            }
        )

    def generate_flow_diagram(
        self, title="Component Interaction Flow", output_file=None
    ):
        """
        Generate a flow diagram showing component interactions

        Args:
            title: Title for the diagram
            output_file: Output file path (default: logs/visuals/flow_{timestamp}.png)

        Returns:
            Path to the generated diagram
        """
        if not output_file:
            timestamp = int(time.time())
            output_file = self.log_dir / f"flow_{timestamp}.png"

        # Create a directed graph
        G = nx.DiGraph()

        # Add nodes for each component (deduplicate)
        agents = {call["agent"] for call in self.agent_calls}
        for handoff in self.handoffs:
            agents.add(handoff["from"])
            agents.add(handoff["to"])

        # Add nodes to the graph
        for agent in agents:
            G.add_node(agent)

        # Add edges for handoffs
        for handoff in self.handoffs:
            # Check if edge already exists
            if G.has_edge(handoff["from"], handoff["to"]):
                # Increment weight
                G[handoff["from"]][handoff["to"]]["weight"] += 1
            else:
                # Create new edge with weight 1
                G.add_edge(handoff["from"], handoff["to"], weight=1)

        # Set up the figure
        plt.figure(figsize=(12, 8))

        # Use a hierarchical layout
        pos = nx.spring_layout(G, seed=42)

        # Draw nodes
        nx.draw_networkx_nodes(
            G, pos, node_color="lightblue", node_size=3000, alpha=0.8
        )

        # Draw edges with varying width based on weight
        edge_width = [G[u][v]["weight"] * 2 for u, v in G.edges()]
        nx.draw_networkx_edges(
            G,
            pos,
            width=edge_width,
            alpha=0.7,
            edge_color="gray",
            arrows=True,
            arrowsize=20,
            arrowstyle="-|>",
        )

        # Draw labels
        nx.draw_networkx_labels(G, pos, font_size=12, font_family="sans-serif")

        # Draw edge labels (number of handoffs)
        edge_labels = {(u, v): f"{G[u][v]['weight']} handoffs" for u, v in G.edges()}
        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=10)

        # Add title and other details
        plt.title(title, fontsize=16)

        # Add token usage information
        if self.token_usage:
            token_text = "Token Usage:\n"
            for model, tokens in self.token_usage.items():
                token_text += f"{model}: {tokens:,} tokens\n"
            plt.figtext(0.02, 0.02, token_text, fontsize=10)

        # Add timestamp
        timestamp_str = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        plt.figtext(0.98, 0.02, f"Generated: {timestamp_str}", fontsize=8, ha="right")

        # Remove axes
        plt.axis("off")

        # Save the figure
        plt.tight_layout()
        plt.savefig(output_file, dpi=100, bbox_inches="tight")
        plt.close()

        return output_file

    def generate_timeline_diagram(self, title="Execution Timeline", output_file=None):
        """
        Generate a timeline diagram showing when components were called

        Args:
            title: Title for the diagram
            output_file: Output file path

        Returns:
            Path to the generated diagram
        """
        if not output_file:
            timestamp = int(time.time())
            output_file = self.log_dir / f"timeline_{timestamp}.png"

        # Get all component names
        agent_names = {call["agent"] for call in self.agent_calls}

        # Create figure
        fig, ax = plt.subplots(figsize=(15, 8))

        # Assign y-positions to components
        agent_positions = {agent: i for i, agent in enumerate(sorted(agent_names))}

        # Set the first call time as time zero
        if self.agent_calls:
            t0 = min(call["timestamp"] for call in self.agent_calls)
        else:
            t0 = time.time()

        # Draw timeline bars for each component call
        colors = plt.cm.tab10.colors

        for i, call in enumerate(
            sorted(self.agent_calls, key=lambda x: x["timestamp"])
        ):
            agent = call["agent"]
            start_time = call["timestamp"] - t0
            duration = call["duration"]
            color = colors[agent_positions[agent] % len(colors)]

            # Draw bar representing call duration
            ax.barh(
                agent_positions[agent],
                duration,
                left=start_time,
                height=0.5,
                color=color,
                alpha=0.7,
                label=agent if i == 0 else "",
            )

            # Add text for token usage
            if call.get("tokens"):
                total_tokens = sum(call["tokens"].values())
                if total_tokens > 0:
                    ax.text(
                        start_time + duration / 2,
                        agent_positions[agent],
                        f"{total_tokens:,} tokens",
                        ha="center",
                        va="center",
                        fontsize=8,
                        color="black",
                    )

        # Draw handoff arrows
        for handoff in self.handoffs:
            from_agent = handoff["from"]
            to_agent = handoff["to"]
            time_point = handoff["timestamp"] - t0

            # Draw arrow
            arrow = FancyArrowPatch(
                (time_point, agent_positions[from_agent]),
                (time_point, agent_positions[to_agent]),
                arrowstyle="-|>",
                mutation_scale=15,
                color="red",
                linewidth=1.5,
                alpha=0.7,
            )
            ax.add_patch(arrow)

        # Set labels and title
        ax.set_yticks(list(agent_positions.values()))
        ax.set_yticklabels(list(agent_positions.keys()))
        ax.set_xlabel("Time (seconds)")
        ax.set_title(title)

        # Set grid
        ax.grid(True, axis="x", linestyle="--", alpha=0.3)

        # Add timestamp
        timestamp_str = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        plt.figtext(0.98, 0.02, f"Generated: {timestamp_str}", fontsize=8, ha="right")

        # Save the figure
        plt.tight_layout()
        plt.savefig(output_file, dpi=100, bbox_inches="tight")
        plt.close()

        return output_file

    def generate_report(self, output_file=None):
        """
        Generate a JSON report with all tracked data

        Args:
            output_file: Output file path

        Returns:
            Path to the generated report
        """
        if not output_file:
            timestamp = int(time.time())
            output_file = self.log_dir / f"report_{timestamp}.json"

        report = {
            "timestamp": time.time(),
            "agent_calls": self.agent_calls,
            "api_calls": self.api_calls,
            "handoffs": self.handoffs,
            "token_usage": self.token_usage,
            "summary": {
                "total_components": len({call["agent"] for call in self.agent_calls}),
                "total_calls": len(self.agent_calls),
                "total_handoffs": len(self.handoffs),
                "total_api_calls": len(self.api_calls),
                "total_tokens": (
                    sum(self.token_usage.values()) if self.token_usage else 0
                ),
            },
        }

        with open(output_file, "w") as f:
            json.dump(report, f, indent=2)

        return output_file


# Singleton instance
_visualizer_instance = None


def get_visualizer():
    """Get the visualizer singleton instance"""
    global _visualizer_instance
    if _visualizer_instance is None:
        _visualizer_instance = AgentVisualizer()
    return _visualizer_instance


def initialize_visualizer(log_dir=None):
    """Initialize the visualizer singleton"""
    global _visualizer_instance
    _visualizer_instance = AgentVisualizer(log_dir)
    return _visualizer_instance

================
File: requirements.txt
================
openai>=1.78.1    # Core OpenAI API
openai-agents>=0.0.14  # OpenAI Agents Python SDK
pydantic>=2.10,<3  # For data validation
requests==2.31.0   # HTTP client
beautifulsoup4==4.12.3  # For HTML parsing
pandas==2.1.4      # For data handling
python-dotenv==1.0.1  # Environment variable management
tabulate==0.9.0    # For Markdown table generation
matplotlib>=3.5.0  # For visualization diagrams
networkx>=3.4      # For agent flow visualization

================
File: build-and-run.sh
================
#!/bin/bash
# ---------------------------------------------------------------
# build-and-run.sh - Build and run the Deep Job Search container
# ---------------------------------------------------------------

# Set strict error handling
set -e

# Create required directories
mkdir -p logs/debug
mkdir -p results

echo "=== Deep Job Search - Build & Run ==="

# Check if we need to rebuild the container
REBUILD=false
if [ -f .rebuild-marker ]; then
  echo "Rebuild marker found (.rebuild-marker), will rebuild container"
  REBUILD=true
  rm .rebuild-marker
fi

# Also check if Dockerfile has been modified
if [ "$(find Dockerfile -newer logs/debug/.last_build 2>/dev/null)" ]; then
  echo "Dockerfile has been modified since last build, will rebuild container"
  REBUILD=true
fi

# Also check if requirements files have been modified
if [ "$(find requirements*.txt -newer logs/debug/.last_build 2>/dev/null)" ]; then
  echo "Requirements files have been modified, will rebuild container"
  REBUILD=true
fi

# Ensure we have a Docker image
if ! docker image inspect djso 2>/dev/null >/dev/null; then
  echo "Docker image 'djso' not found, will build it"
  REBUILD=true
fi

# Build the Docker image if needed
if [ "$REBUILD" = true ]; then
  echo "Building Docker image..."
  docker build -t djso .
  touch logs/debug/.last_build
fi

# Check if output arg is already specified
HAS_OUTPUT=false
# Check if simplified flag is present
HAS_SIMPLIFIED=false
for arg in "$@"; do
  if [[ "$arg" == "--output" ]]; then
    HAS_OUTPUT=true
  fi
  if [[ "$arg" == "--simplified" ]]; then
    HAS_SIMPLIFIED=true
  fi
done

# Build the argument array for run.sh
ARGS=("$@")

# Add output argument if needed
if [ "$HAS_OUTPUT" = false ]; then
  ARGS+=("--output" "results/jobs.csv")
fi

# Run the tool
echo "Running Deep Job Search in Docker container..."

# Prepare arguments as a string for docker command
ARG_STRING=""
for arg in "${ARGS[@]}"; do
  ARG_STRING="$ARG_STRING \"$arg\""
done

# Execute the container with the arguments
docker run --rm \
  -e OPENAI_API_KEY \
  -e RUNNING_IN_CONTAINER=1 \
  -v "$(pwd)/results:/app/results" \
  -v "$(pwd)/logs:/app/logs" \
  --entrypoint /bin/bash \
  djso -c "cd /app && ./run.sh $ARG_STRING"

# Check exit code
STATUS=$?
if [ $STATUS -eq 0 ]; then
  echo "Job search completed successfully!"
else
  echo "Job search failed with exit code $STATUS"
fi

exit $STATUS

================
File: README.md
================
# Deep Job Search

An AI-powered job search tool that uses OpenAI's Agents SDK with a specialized multi-agent architecture to find software engineering jobs.

## What's New: Multi-Agent Architecture & Simplified API

We've completely redesigned the system with a specialized multi-agent architecture and added a simplified API:

- **Multi-Agent System**: Four specialized agents (Planner, Searcher, Processor, Verifier) that work together
- **URL Validation**: Advanced multi-stage validation ensures we only return real jobs
- **Pattern Recognition**: Intelligent analysis of job listing URLs to filter out invalid links
- **Simplified API**: New streamlined interface for easier integration and quicker searches
- **Robust Operation**: Improved error handling and fallback mechanisms for reliable results
- **Web Verification**: Enhanced web search verification of job URLs for higher quality results

## Features

- Multi-agent system with specialized planning, searching, processing, and verification
- Finds jobs at both major tech companies and promising startups
- Returns structured job data with titles, companies, and verified application URLs
- Customizable search parameters (number of jobs, company type, etc.)
- Detailed logging and visualization of the search process

## Usage

### Basic Usage

Use the `run.sh` script to search for jobs:

```bash
./run.sh --majors 20 --startups 20
```

For a faster, streamlined experience, use the simplified mode:

```bash
./run.sh --simplified
```

### Options

- `-m, --majors COUNT`: Number of major company jobs to find (default: 10)
- `-s, --startups COUNT`: Number of startup jobs to find (default: 10)
- `--model MODEL`: Model to use (default: gpt-4o)
- `--simplified`: Use the streamlined agent interface
- `--company-list FILE`: Custom CSV file with company names
- `--use-web-verify`: Enable web search verification for higher quality
- `--debug`: Enable debug mode with more logging
- `--output FILE`: Output file path (default: results/jobs.csv)

### Examples

Find 15 major company jobs and 5 startup jobs:

```bash
./run.sh --majors 15 --startups 5
```

Use simplified mode with a lighter model for faster results:

```bash
./run.sh --simplified --model gpt-4o-mini
```

## Requirements

- Python 3.9+
- OpenAI API key (set as OPENAI_API_KEY environment variable)
- Required Python packages (see requirements.txt)

## Installation

1. Clone this repository
2. Install dependencies: `pip install -r requirements.txt`
3. Set your OpenAI API key:
   ```bash
   export OPENAI_API_KEY=your-api-key
   ```
4. Run the job search: `./run.sh`

## Understanding the Results

The tool saves job results to a CSV file (default: `results/jobs.csv`) with fields:
- `#`: Job number
- `title`: Job title
- `company`: Company name
- `url`: URL to job application
- `type`: "Major" or "Startup"
- `has_apply`: Whether the job URL has been verified to contain an application
- `found_date`: Date the job was found

## Important Notes

- **Quality vs. Quantity**: The tool prioritizes finding real jobs with working URLs over meeting quotas
- **URL Validation**: All returned jobs have undergone URL validation to ensure they're real
- **Web Verification**: The `--web-verify` option (enabled by default) uses web search to verify job URLs
- **Finding Fewer Jobs**: If fewer jobs are found than requested, it's because:
  1. Some job URLs failed validation (we only return verified jobs)
  2. The search didn't find enough relevant jobs

To improve results, try:
1. Using different search keywords
2. Specifying a custom company list
3. Enabling web verification for more accurate results

## Using This Tool

This tool uses the OpenAI Responses API with web search capabilities to efficiently find job listings for software engineering roles in the video and streaming industry.

You can run the application using the provided script:

```bash
# Run with default settings
./build-and-run.sh

# Quick sample run with minimal jobs
./build-and-run.sh --sample
```

The application automatically uses Docker for consistent execution across environments.

## System Architecture

```mermaid
graph TD
    User[User] --> |starts| DeepJobSearch[Deep Job Search]
    DeepJobSearch --> |initializes| AgentSystem[Multi-Agent System]

    %% Agent definitions
    subgraph AgentSystem[Multi-Agent System]
        Planner[Planner Agent] --> |creates search plan| Searcher[Searcher Agent]
        Searcher --> |uses| WebSearchTool[Web Search Tool]
        Searcher --> |extracts listings| Processor[Processor Agent]
        Processor --> |validates URLs| Verifier[Verifier Agent]
    end

    WebSearchTool --> |searches| Companies[Major & Startup Companies]
    Companies --> |job listings| Searcher
    Verifier --> |validates| JobListings[Validated Job Listings]
    JobListings --> |saved as| Results[CSV/MD Results]
```

The system uses a specialized multi-agent architecture with OpenAI's Agents SDK:

1. **Planner Agent** - Creates optimal search strategies by pairing companies with relevant keywords
2. **Searcher Agent** - Performs web searches and extracts job information using specialized tools
3. **Processor Agent** - Processes search results into structured job listings
4. **Verifier Agent** - Validates job URLs to ensure they're real postings with application forms

This multi-agent approach allows each component to specialize in its task while maintaining a coordinated workflow that delivers high-quality, verified job listings.

## Project Structure

The project consists of the following key files:

- **deep_job_search.py** - Main Python script implementing the multi-agent job search architecture. Features:
  - Planner, Searcher, Processor and Verifier agent pipeline
  - Integration with OpenAI's Agents SDK
  - Advanced URL validation and verification
  - Simplified API interface for easier integration
  - Fall-back mechanisms for robust operation
- **api_wrapper.py** - Utility wrapper for OpenAI API calls with logging and timing metrics. Provides consistent error handling and statistics tracking.
- **logger_utils.py** - Enhanced logging utility for detailed execution tracking, with features for nested operations, API call logging, and timing analysis.
- **agent_visualizer.py** - Visualization tool that generates diagrams and reports showing API call flow and token usage.
- **build-and-run.sh** - Main script for building and running the Docker container. This is the primary entry point for running the application.
- **run.sh** - Simple alias to build-and-run.sh for backward compatibility. Only maintained for scripts that might reference it.
- **Dockerfile** - Docker configuration for containerizing the application
- **requirements.txt** - Python dependencies
- **container_setup.md** - Documentation for Docker container setup
- **roles_200.csv/md** - Reference data for job roles
- **jobbot_costs.png** - Chart visualizing cost comparison between different OpenAI models for running searches with 200 roles

The application produces output in two locations:
- **results/** - Contains job search results in CSV and Markdown formats
- **logs/** - Contains detailed execution logs and API call information

## Docker Support

The application is designed to run in Docker to ensure consistent execution across environments. The `build-and-run.sh` script handles all Docker operations automatically:

```bash
# Build and run with Docker (first time or with --rebuild flag)
./build-and-run.sh

# Run with specific options
./build-and-run.sh --sample --model gpt-4o
```

For more details about the Docker setup, see [container_setup.md](container_setup.md).

## CLI Options

```
usage: ./build-and-run.sh [options]

Main options:
  --help                show this help message and exit
  --majors N            Number of major company jobs to find
  --startups N          Number of startup company jobs to find
  --sample              Run with minimal settings (2 major, 2 startup jobs)
  --simplified          Run using the simplified agent interface
  --log-level LEVEL     Logging level (default: INFO)
  --log-file FILE       Log to this file in addition to console
  --force               Skip cost confirmation prompt
  --budget BUDGET       Maximum cost in USD (exit if exceeded)
  --company-list-limit  Maximum number of companies to display in prompts
  --use-web-verify      Use web search for URL verification
  --rebuild             Force rebuild of the Docker image
  --visualize           Generate visualization diagrams (default: True)
  --no-visualize        Disable visualization generation

Model options:
  --model MODEL         Model for agent system (default: gpt-4o)
```

### Simplified Mode

The system now includes a simplified interface mode that streamlines the job search process:

```bash
# Run in simplified mode
./build-and-run.sh --simplified

# Combine with other options
./build-and-run.sh --simplified --model gpt-4o-mini
```

This mode uses a more direct approach with fewer API calls, making it faster but potentially less thorough than the full multi-agent system.

> **Note:** The `--use-web-verify` option is included for future expansion but currently has limited implementation. It's intended to verify job URLs via web search but this functionality is not fully integrated.

## Performance Optimization

### Model Selection for Cost/Performance Balance

Different models offer varying balances of cost, speed, and quality:

```bash
# Economy mode - Lowest cost (~$0.008 per run)
./build-and-run.sh --model gpt-3.5-turbo

# Standard mode - Good balance (~$0.03 per run)
./build-and-run.sh --model gpt-4o

# Performance mode - Highest quality (~$0.08 per run)
./build-and-run.sh --model gpt-4.1
```

### Sample Mode for Development

When developing or testing the system, use sample mode for faster, cheaper runs:

```bash
# Sample mode with only 2 major and 2 startup jobs
./build-and-run.sh --sample

# Sample mode with a specific model
./build-and-run.sh --sample --model gpt-3.5-turbo
```

### Logging Configuration for Debugging

Adjust logging for different needs:

```bash
# Minimal logging for production
./build-and-run.sh --log-level WARNING

# Standard logging
./build-and-run.sh --log-level INFO

# Verbose logging for debugging
./build-and-run.sh --log-level DEBUG

# Custom log file
./build-and-run.sh --log-file custom_log.txt
```

### Budget Control

Set explicit budget controls:

```bash
# Set a maximum budget of $0.10
./build-and-run.sh --budget 0.10

# Skip the cost confirmation prompt
./build-and-run.sh --force

# Estimate costs without running
JOBBOT_ESTIMATE_ONLY=1 ./build-and-run.sh
```

## Cost Estimation

The application uses the OpenAI Responses API with the following pricing tiers:

| Model | Price per 1K tokens | Typical Usage | Estimated Cost |
|-------|---------------------|---------------|----------------|
| gpt-4o | $0.005 | ~8K tokens | $0.04 |
| gpt-4o-mini | $0.002 | ~8K tokens | $0.016 |
| gpt-3.5-turbo | $0.001 | ~8K tokens | $0.008 |

Actual costs will vary based on:
- Number of jobs searched (controlled by `--majors` and `--startups` flags)
- Complexity of search results
- The model used (controlled by `--model` flag)

To control costs, you can:
- Limit the number of major/startup jobs with `--majors` and `--startups` flags
- Use less expensive models with the `--model` flag
- Set a token usage limit with `--max-tokens`

The `jobbot_costs.png` file visualizes the cost differences between models for a standard 200-role search.

### Cost Comparison Examples

| Configuration | Estimated Cost |
|---------------|----------------|
| Default (10+10 jobs) | $0.04 |
| Economy mode (--model gpt-3.5-turbo) | $0.008 |
| Sample mode (2+2 jobs) | $0.016 |
| Premium mode (--model gpt-4.1) | $0.08 |

## Results

Results are saved in the `results` directory as both CSV and Markdown files. Logs are saved in the `logs` directory.

Each result includes:
- Job number (#)
- Job title
- Company name
- Company type (Major/Startup)
- Job URL

### Example Output

CSV format:
```csv
#,title,company,type,url
1,Senior Software Engineer,Netflix,Major,https://netflix.com/jobs/123
2,Backend Developer,Mux,Startup,https://mux.com/careers/456
```

Markdown format:
```markdown
# Job Search Results

Found 2 jobs.

|   # | title                   | company   | type   | url                           |
|-----|-------------------------|-----------|--------|-------------------------------|
|   1 | Senior Software Engineer | Netflix   | Major  | https://netflix.com/jobs/123 |
|   2 | Backend Developer       | Mux       | Startup | https://mux.com/careers/456  |
```

## Requirements

- Python 3.9+
- OpenAI API key (set as OPENAI_API_KEY environment variable)
- Required Python packages (see requirements.txt)

## Installation

### Prerequisites

- Docker
- OpenAI API Key

### Setup

Clone this repository:

```bash
git clone <repository-url>
cd deep-job-search
```

Create a `.env` file with your OpenAI API key:

```
OPENAI_API_KEY=your_api_key_here
```

## Usage Examples

### Basic Usage

```bash
# Default settings (10 major company and 10 startup jobs)
./build-and-run.sh

# Quick sample run (2 major and 2 startup jobs)
./build-and-run.sh --sample

# Custom job counts
./build-and-run.sh --majors 20 --startups 10

# Limit company list length in prompts
./build-and-run.sh --company-list-limit 5
```

### Model Selection

```bash
# Use different models for different use cases
./build-and-run.sh --model gpt-4o           # Default (balanced)
./build-and-run.sh --model gpt-4o-mini      # Economy (faster, cheaper)
./build-and-run.sh --model gpt-4.1          # Premium (highest quality)
./build-and-run.sh --model gpt-3.5-turbo    # Lowest cost option
```

### Comprehensive Examples

```bash
# Economy run - Lowest cost option (~$0.008)
./build-and-run.sh --model gpt-3.5-turbo --majors 5 --startups 5 --log-level WARNING --no-visualize

# Standard run - Balanced cost/performance (~$0.04)
./build-and-run.sh --model gpt-4o --majors 10 --startups 10 --company-list-limit 10

# High performance run - Better quality within $0.25 budget
./build-and-run.sh --model gpt-4.1 --majors 15 --startups 10 --force --budget 0.25 --use-web-verify

# Maximum performance run - All options with $25 budget limit
./build-and-run.sh --model gpt-4.1 --majors 50 --startups 50 --force --budget 25.00 --log-level DEBUG --max-tokens 500000 --company-list-limit 20 --visualize --use-web-verify
```

### Feature-specific Examples

```bash
# Run with budget control
./build-and-run.sh --budget 0.10 --force

# Estimate costs without running
JOBBOT_ESTIMATE_ONLY=1 ./build-and-run.sh --majors 20 --startups 20 --model gpt-4.1

# Force rebuild Docker image with sample run
./build-and-run.sh --rebuild --sample

# Custom log file with increased verbosity
./build-and-run.sh --log-level DEBUG --log-file custom_jobsearch.log

# Disable visualizations for faster execution
./build-and-run.sh --no-visualize

# Maximum company list detail in prompts
./build-and-run.sh --company-list-limit 20

# Skip all confirmations with environment variable
JOBBOT_SKIP_CONFIRM=1 ./build-and-run.sh --model gpt-4.1 --majors 30 --startups 20
```

### Debugging and Development

```bash
# Verbose debugging with trace output
./build-and-run.sh --log-level DEBUG --trace

# Testing with future web verification feature
./build-and-run.sh --sample --use-web-verify

# Development quick test with debug logs
./build-and-run.sh --sample --log-level DEBUG --rebuild

# Full test with all features
./build-and-run.sh --model gpt-4.1 --majors 5 --startups 5 --log-level DEBUG --trace --company-list-limit 15 --max-tokens 100000 --visualize --force
```

### All Options Example

```bash
# Example with every single CLI option specified
./build-and-run.sh \
  --model gpt-4.1 \
  --majors 25 \
  --startups 25 \
  --max-tokens 200000 \
  --budget 25.00 \
  --force \
  --log-level DEBUG \
  --log-file comprehensive_job_search.log \
  --trace \
  --company-list-limit 20 \
  --visualize \
  --rebuild \
  --use-web-verify
```

### Logging and Debugging

```bash
# Run with verbose logging
./build-and-run.sh --log-level DEBUG

# Save logs to a specific file
./build-and-run.sh --log-file custom_log.txt

# Enable trace output for detailed execution tracking
./build-and-run.sh --trace
```

### Visualization

The tool automatically generates visualization diagrams showing API calls, token usage, and process flow:

```bash
# Run with visualization enabled (default)
./build-and-run.sh

# Disable visualization for faster execution
./build-and-run.sh --no-visualize
```

Visualization files are saved to the `logs/visuals` directory:
- **Timeline diagrams**: Show when each API call was made and how long it took
- **Execution reports**: Contain detailed information about token usage and API calls

### Cost Management

```bash
# Set token limit for tighter cost control
./build-and-run.sh --max-tokens 50000

# Skip confirmation prompt
./build-and-run.sh --force

# Set a maximum budget
./build-and-run.sh --budget 0.10
```

### Docker Management

```bash
# Force rebuild of Docker image
./build-and-run.sh --rebuild

# Run sample mode with rebuild
./build-and-run.sh --sample --rebuild
```

### Environment Variables

You can set these environment variables:

```bash
# Skip confirmation prompts
export JOBBOT_SKIP_CONFIRM=1

# Exit after cost estimation
export JOBBOT_ESTIMATE_ONLY=1

# Run with specified variables
JOBBOT_SKIP_CONFIRM=1 ./build-and-run.sh
```

## Output

Results are saved in the `results` directory:

- `job_results.csv` - CSV format suitable for spreadsheet applications
- `job_results.md` - Markdown format for easy viewing

Each result includes:
- Job number (#)
- Job title
- Company name
- Company type (Major/Startup)
- Job URL

### Example Output

CSV format:
```csv
#,title,company,type,url
1,Senior Software Engineer,Netflix,Major,https://netflix.com/jobs/123
2,Backend Developer,Mux,Startup,https://mux.com/careers/456
```

Markdown format:
```markdown
# Job Search Results

Found 2 jobs.

|   # | title                   | company   | type   | url                           |
|-----|-------------------------|-----------|--------|-------------------------------|
|   1 | Senior Software Engineer | Netflix   | Major  | https://netflix.com/jobs/123 |
|   2 | Backend Developer       | Mux       | Startup | https://mux.com/careers/456  |
```

## Developer API Integration

The system now provides a clean API for integrating job search capabilities into your applications. The simplified `gather_jobs` function offers an easy entry point:

```python
from deep_job_search import gather_jobs
import asyncio

async def search_for_jobs():
    # Define companies and keywords
    companies = ["Google", "Microsoft", "Amazon", "Mux", "Livepeer", "Daily.co"]
    keywords = ["video", "encoding", "streaming"]

    # Perform the search (returns list of JobListing objects)
    jobs = await gather_jobs(
        companies=companies,
        keywords=keywords,
        model="gpt-4o-mini",  # Use a lighter model for faster results
        max_jobs=10           # Limit total jobs returned
    )

    # Process job results
    for job in jobs:
        print(f"{job.title} at {job.company} - {job.url}")

    return jobs

# Run the search
jobs = asyncio.run(search_for_jobs())
```

For more advanced usage, you can access the full multi-agent system with `gather_jobs_with_multi_agents()`, which provides more configurable options and detailed control over the search process.

## Troubleshooting

### API Key Issues

- Ensure your OPENAI_API_KEY is set correctly in the .env file
- Verify your API key has access to the model you're trying to use

```bash
# Test your API key
echo $OPENAI_API_KEY
```

### Connection Issues

- Check your internet connection
- Increase logging with `--log-level DEBUG` to see detailed error messages

```bash
# Check Docker container access to the internet
./build-and-run.sh --log-level DEBUG
```

### Model Availability

If you encounter errors about model availability:

```bash
# Try using a more widely available model
./build-and-run.sh --model gpt-4o
```

### Docker Issues

If Docker isn't working properly:

```bash
# Force rebuild the Docker image
./build-and-run.sh --rebuild

# Check Docker status
docker info
```

## Advanced Configuration

### Fine-tuning the Search

You can adjust how the search works by modifying the query formation in `deep_job_search.py`:

```python
query = f"""
I need you to find {count} software engineering job listings at major companies in the video/streaming industry.

Focus on these companies: {companies}
Look for roles with keywords like: {keywords}

For each job listing, I need:
1. The exact job title
2. The company name
3. The direct URL to the job posting (not a careers page)

Return the results as a structured JSON array with fields: title, company, url, type
"""
```

### Customizing Output Formats

By default, results are saved in two formats:
1. **CSV** (`job_results.csv`): Excel and spreadsheet compatible
2. **Markdown** (`job_results.md`): GitHub and documentation friendly

You can customize the format by modifying the `save()` function in `deep_job_search.py`.

## License

[MIT License](LICENSE)

================
File: deep_job_search.py
================
#!/usr/bin/env python
"""
deep_job_search.py — Deep Job Search with Multi-Agent implementation

This file uses the OpenAI Agents SDK for an advanced, multi-agent job search
architecture. It implements a pipeline of specialized agents:
1. Planner - Creates search strategies
2. Searcher - Performs web searches for jobs
3. Processor - Processes and extracts structured job data
4. Verifier - Validates job URLs and application processes
"""

import os
import re
import json
import time
import argparse
import logging
import sys
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse

import pandas as pd
from dotenv import load_dotenv
from openai import OpenAI
from pydantic import BaseModel
from agents import Agent, Runner, function_tool, ModelSettings, WebSearchTool, usage as agent_usage

# Import our utility modules
from logger_utils import setup_enhanced_logger, DepthContext
from api_wrapper import initialize_api_wrapper
from agent_visualizer import initialize_visualizer, get_visualizer

load_dotenv()

# Constants and configuration
OUTPUT_DIR = Path(__file__).with_suffix("")
RESULTS_DIR = Path("results")  # For Docker compatibility

# Companies and keywords lists
MAJOR_COMPANIES = [
    "Amazon",
    "Apple",
    "Google",
    "Meta",
    "Microsoft",
    "Netflix",
    "NVIDIA",
    "OpenAI",
    "Anthropic",
    "Disney",
    "Hulu",
    "TikTok",
    "Snap",
    "Zoom",
    "Cisco",
    "Comcast",
    "Warner Bros. Discovery",
    "Paramount",
    "IBM",
    "Valve",
]
STARTUP_COMPANIES = [
    "Livepeer",
    "Twelve Labs",
    "Tapcart",
    "Mux",
    "Apporto",
    "GreyNoise",
    "Daily.co",
    "Temporal Technologies",
    "Bitmovin",
    "JWPlayer",
    "Brightcove",
    "Cloudflare",
    "Fastly",
    "Firework",
    "Bambuser",
    "FloSports",
    "StageNet",
    "Uscreen",
    "Mmhmm",
    "StreamYard",
    "Agora",
    "Conviva",
    "Peer5",
    "Wowza",
    "Hopin",
    "Kumu Networks",
    "Touchcast",
    "Theo Technologies",
    "Vidyard",
    "Cinedeck",
    "InPlayer",
    "Netlify",
    "Vowel",
    "Edge-Next",
    "Streann Media",
    "Gather",
    "Frequency",
    "Truepic",
    "LiveControl",
    "OpenSelect",
]
KEYWORDS = [
    "video",
    "streaming",
    "media",
    "encoding",
    "cloud",
    "kubernetes",
    "backend",
    "principal",
    "lead",
    "infrastructure",
]


# ------------- Data models -------------
class JobSearchPair(BaseModel):
    """A company-keyword pair for job searching"""
    company: str
    keyword: str

class JobListing(BaseModel):
    """A job listing with relevant details"""
    title: str
    company: str
    type: str  # Major or Startup
    url: str
    has_apply: bool = False
    found_date: Optional[str] = None


# ------------- logging -------------
def setup_logger(level: str = "INFO", file: str | None = None) -> logging.Logger:
    """Set up the enhanced logger with improved format and API logging capabilities."""
    # Create API log file path if a main log file is provided
    api_log_file = None
    if file:
        api_log_file = Path(file).with_suffix(".api.log")

    # Create logs directory structure
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)

    visuals_dir = logs_dir / "visuals"
    visuals_dir.mkdir(exist_ok=True)

    # Initialize the visualizer
    initialize_visualizer(visuals_dir)

    # Set up the enhanced logger
    logger = setup_enhanced_logger(level=level, file=file, api_log_file=api_log_file)

    # Initialize the API wrapper with our logger
    initialize_api_wrapper(logger)

    # Log diagnostic info
    logger.debug(f"Logger initialized with level: {level}")
    logger.debug(f"Python version: {sys.version}")

    return logger


class Timer:
    """Enhanced timer context manager with more detailed timing info using DepthContext."""

    def __init__(self, label, logger, log_level="INFO"):
        self.label = label
        self.logger = logger
        self.log_level = log_level
        self.depth_context = None

    def __enter__(self):
        self.depth_context = DepthContext(self.logger, self.label, self.log_level)
        self.depth_context.__enter__()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.depth_context.__exit__(exc_type, exc_val, exc_tb)
        if exc_type:
            self.logger.error(f"ERROR in {self.label}: {exc_val}")


# ------------- token monitoring -------------
class TokenMonitor:
    """Track and budget token usage across different phases with enhanced logging"""

    # Cost per 1K tokens (as of 2025)
    COST_PER_1K = {
        "gpt-4.1": 0.01,
        "gpt-4o": 0.005,
        "gpt-4o-mini": 0.002,
        "gpt-3.5-turbo": 0.001,
        "o3": 0.0005,
    }

    def __init__(self, cfg, logger):
        self.cfg = cfg
        self.logger = logger
        self.max_tokens = cfg.get("max_tokens", 100000)
        self.total_tokens_used = 0

    def get_model_rate(self, model: str) -> float:
        """Get the cost rate for a model, with fallback for unknown models"""
        base_model = model.split("-preview")[0].split("-vision")[0]
        return self.COST_PER_1K.get(
            base_model, 0.005
        )  # Default to mid-tier pricing if unknown

# ------------- Agent tools -------------
@function_tool
def validate_job_url(url: str) -> bool:
    """
    Validate if a job URL is likely to be a real job posting

    Args:
        url: The job URL to validate

    Returns:
        bool: True if the URL appears to be a valid job posting, False otherwise
    """
    try:
        # Check if URL is properly formatted
        parsed_url = urlparse(url)
        if not all([parsed_url.scheme, parsed_url.netloc]):
            return False

        # Check for common job board domains and patterns
        job_domains = [
            "linkedin.com/jobs", "indeed.com/job", "glassdoor.com/job",
            "lever.co", "greenhouse.io", "workday.com", "smartrecruiters.com",
            "jobs.", "careers.", "apply.", "/job/", "/jobs/"
        ]

        # Check for suspicious patterns
        suspicious_patterns = [
            "example.com", "test", "dummy", "placeholder",
            "{company}", "{role}", "{id}", "{keyword}"
        ]

        # Check domain and path
        url_lower = url.lower()

        # Return False if any suspicious pattern is found
        if any(pattern in url_lower for pattern in suspicious_patterns):
            return False

        # Return True if any job domain pattern is found
        return any(pattern in url_lower for pattern in job_domains)

    except Exception:
        return False

@function_tool
async def extract_job_listings(query: str) -> str:
    """
    Extract job listings from search results

    Args:
        query: The search query text

    Returns:
        str: JSON string of extracted job listings
    """
    # Define regex patterns for job listing extraction
    job_patterns = [
        # Standard job listing pattern with title and company
        r'(?i)(senior|junior|staff|principal)?\s*([a-z\s]+)(engineer|developer|architect)\s+(?:at|with|for|@)\s+([a-z0-9\s\.,]+)\s*(?:-|–|:)\s*.*?(https?://[^\s"\']+)',

        # Job title with company - common format
        r'(?i)"?([a-z0-9\s\-]+(?:engineer|developer|architect)[a-z0-9\s\-]*)"?\s+(?:at|with|for|@)\s+([a-z0-9\s\.,\-]+)\s*(?:\(|:|\.|,|\n|\[|\-|–)\s*(https?://[^\s"\'\)]+)',

        # Company careers page with job titles
        r'(?i)([a-z0-9\s\.,\-]+)\s+(?:careers|jobs|positions|roles)\s+(?:available|open).*?(https?://[^\s"\'\)]+careers|jobs|positions)',

        # URL with job in path
        r'(https?://[^\s"\']+(?:job|career)[^\s"\']*)',

        # Job board URL with typical parameters
        r'(https?://(?:www\.)?(?:linkedin\.com|indeed\.com|glassdoor\.com|lever\.co|greenhouse\.io|workday\.com)\/[^\s"\']+)'
    ]

    all_matches = []

    # Apply each pattern and collect matches
    for pattern in job_patterns:
        matches = re.findall(pattern, query)
        if matches:
            all_matches.extend(matches)

    # Process and format results
    results = []
    for match in all_matches:
        # Handle different match formats
        if isinstance(match, tuple):
            if len(match) >= 5:  # First pattern
                title = f"{match[0]} {match[1]} {match[2]}".strip()
                company = match[3].strip()
                url = match[4].strip()
            elif len(match) >= 3:  # Second and third patterns
                if "careers" in match[2] or "jobs" in match[2]:
                    # Company careers page
                    company = match[0].strip()
                    title = "Multiple Positions"
                    url = match[2].strip()
                else:
                    # Standard job listing
                    title = match[0].strip()
                    company = match[1].strip()
                    url = match[2].strip()
            else:
                continue
        else:
            # Single URL match
            url = match.strip()
            title = "Job Listing"
            company = urlparse(url).netloc.replace("www.", "")

        # Basic validation of extracted URL
        if validate_job_url(url):
            results.append({
                "title": title.strip(),
                "company": company.strip(),
                "url": url.strip()
            })

    # Return formatted results as JSON string
    return json.dumps(results, indent=2)

@function_tool
async def verify_job_url(url: str) -> bool:
    """
    Verify if a job URL is valid by checking URL patterns

    Args:
        url: The job URL to verify

    Returns:
        bool: True if the URL is a valid job posting, False otherwise
    """
    # Perform basic validation only
    if not validate_job_url(url):
        return False

    # Basic validation passed
    return True

# ------------- Agent prompts -------------
def format_company_list(companies, limit=10):
    """Format a list of companies for the prompt"""
    if limit and len(companies) > limit:
        return ", ".join(companies[:limit]) + f" and {len(companies) - limit} more"
    return ", ".join(companies)

def planner_prompt(major_companies, startup_companies, keywords) -> str:
    major_companies_str = ", ".join(major_companies)
    startup_companies_str = ", ".join(startup_companies)
    keywords_str = ", ".join(keywords)

    return f"""
## Role
You are a job search planning agent specialized in software engineering roles.

## Task
Create an optimal job search strategy that balances major companies and startups.

## Instructions
1. Create a search plan that pairs companies with relevant keywords
2. Focus on high-quality, strategic company-keyword pairs
3. Balance between major companies and startups
4. Avoid duplicate companies in the plan
5. CRITICAL: Consider only companies that are likely to have REAL job postings

## Major Companies
{major_companies_str}

## Startup Companies
{startup_companies_str}

## Keywords
{keywords_str}

## Output Format
IMPORTANT: You must return ONLY a valid JSON array of objects, with no explanation text before or after:
[
  {{
    "company": "Company Name",
    "keyword": "Keyword"
  }},
  ...
]

## Rules
- Include a diverse mix of companies and keywords
- Focus on promising company-keyword combinations
- Avoid keywords that are too generic or too specific
- Include both major companies and startups
- MOST IMPORTANT: Return ONLY raw JSON - no markdown formatting, no code blocks, no explanations
"""

def searcher_prompt() -> str:
    return """
## Role
You are a job search agent specialized in finding software engineering roles.

## Task
Find job listings for specific companies and keywords using web search.

## Instructions
1. Perform a targeted search for job listings using the company and keyword
2. Focus on finding active, legitimate job postings with application links
3. Extract job information from search results and websites
4. Filter results to find the most relevant technical roles
5. IMPORTANT: Only collect REAL job listings that actually exist
6. Verify that job URLs follow standard patterns for legitimate postings
7. When using search, target career pages and job boards
8. Collect sufficient details about each job for the processor agent

## Strategies
- Search for "[Company] [Keyword] jobs careers apply"
- Search for "[Company] careers [Keyword] engineer apply"
- Look for careers.company.com, jobs.company.com, or company pages on job boards
- Check LinkedIn, Indeed, Glassdoor, and company career pages
- Use detailed searches to find specific role types

## Rules
- Focus on collecting detailed, accurate information
- Prioritize official company career pages and legitimate job boards
- Only extract real job listings with working URLs
- Never fabricate or imagine job listings
- If no results are found, clearly state that no jobs were found
- Pass all relevant search results to the processor

## Available Tools
- Web search
- Job listing extractor
"""

def processor_prompt() -> str:
    return """
## Role
You are a job listing processor agent specialized in extracting structured data.

## Task
Process and extract structured job listings from web search results.

## Instructions
1. Analyze the search results text
2. Extract ONLY REAL job listings including title, company, URL, and type
3. Format each listing with consistent structure
4. Filter out irrelevant results and duplicates
5. Return only relevant technical job postings
6. Ensure URLs are complete and properly formatted
7. IMPORTANT: Always return valid JSON format
8. CRITICAL: NEVER create or invent job listings. Only extract real jobs from the search results.
9. If no relevant job listings are found, return an empty array []

## Output Structure
Each job listing must include these fields:
- "title": The job title (string)
- "company": The company name (string)
- "url": The full URL to the job posting (string)
- "type": The company type, usually "Major" or "Startup" (string)

## Output Format
Return results as a valid JSON array of objects:
```json
[
  {
    "title": "Senior Software Engineer",
    "company": "Example Corp",
    "url": "https://example.com/jobs/12345",
    "type": "Major"
  },
  ...
]
```

## Rules
- Always use double quotes for JSON strings and property names
- Ensure all URLs are valid and complete (not relative)
- Return an empty array [] if no relevant jobs are found
- Do not include explanations or markdown outside the JSON
- NEVER invent or fabricate job listings - only extract real ones
"""

def verifier_prompt() -> str:
    return """
## Role
You are a job listing verification agent.

## Task
Verify if a job URL is valid and contains an apply button or application form.

## Instructions
1. First, analyze the job URL pattern to determine if it's likely a valid job posting
2. Common job URL patterns indicating a valid job:
   - amazon.jobs/en/jobs/12345/title
   - linkedin.com/jobs/view/12345
   - indeed.com/viewjob?jk=12345
   - careers.company.com/position/12345
   - apply.company.com/job/12345
   - jobs.company.com/openings/12345
   - lever.co/company/12345
   - greenhouse.io/company/12345
   - workday.com/company/12345
3. If web search is available, use it to verify the URL by checking:
   - Is this a real job posting page?
   - Does it contain an apply button or application form?
   - Is it from a legitimate company website or job board?
   - Does the page load without errors (no 404, etc.)?
4. Red flags that indicate an invalid job:
   - 404 errors or "job not found" messages
   - Pages that redirect to generic career pages
   - URLs that don't follow standard job listing patterns
   - URLs that contain random placeholders or template patterns
5. CRITICAL: Only mark URLs as valid if they point to REAL job listings

## Output Format
Return "true" if the URL is valid and points to a real job, "false" if not. Lowercase, no explanation.
"""

# ------------- Multi-agent job search implementation -------------
async def gather_jobs_with_multi_agents(cfg, logger) -> List[Dict[str, Any]]:
    """
    Main function that coordinates the multi-agent job search workflow

    Args:
        cfg: Configuration dictionary
        logger: Logger instance

    Returns:
        List of job dictionaries
    """
    # Extract configuration
    majors_quota = cfg.get("majors", 10)
    startups_quota = cfg.get("startups", 10)
    model = cfg.get("model", "gpt-4o")
    company_list_limit = cfg.get("company_list_limit", 10)
    use_web_verification = cfg.get("use_web_verify", False)

    # Log configuration
    logger.info(f"Starting multi-agent job search workflow")
    logger.info(f"Targeting {majors_quota} major company jobs and {startups_quota} startup jobs")
    logger.info(f"Web URL verification: {'Enabled' if use_web_verification else 'Basic validation only'}")
    logger.info(f"Using model: {model}")

    # Initialize agents
    logger.info("Initializing agents...")

    # Create all agents with a more structured approach
    # 1. Planner Agent - Creates search strategies
    planner = Agent(
        name="planner",
        instructions=planner_prompt(MAJOR_COMPANIES, STARTUP_COMPANIES, KEYWORDS),
        model=model,
        model_settings=ModelSettings(temperature=0)
    )

    # 2. Processor Agent - Processes and extracts job data
    processor = Agent(
        name="processor",
        instructions=processor_prompt(),
        model=model,
        model_settings=ModelSettings(temperature=0)
    )

    # 3. Verifier Agent - Validates job URLs
    verifier = Agent(
        name="verifier",
        instructions=verifier_prompt(),
        model=model,
        model_settings=ModelSettings(temperature=0)
    )

    # Set up the processor to hand off to verifier
    processor.handoffs = [verifier]

    # 4. Searcher Agent - Performs web searches with tools
    searcher = Agent(
        name="searcher",
        instructions=searcher_prompt(),
        tools=[WebSearchTool(), extract_job_listings, validate_job_url],
        model=model,
        model_settings=ModelSettings(temperature=0),
        handoffs=[processor]
    )

    # Get visualizer
    visualizer = get_visualizer()

    # Execute planning phase
    with Timer("Planning search strategies", logger):
        logger.info("Planning search strategies...")
        plan_start_time = time.time()
        plan_result = await Runner.run(
            planner,
            input="Generate a job search plan focusing on both major and startup companies"
        )
        plan_json = plan_result.final_output
        plan_duration = time.time() - plan_start_time

        # Track planning step in visualizer
        try:
            # Get token information if available
            tokens_used = {}
            if hasattr(plan_result, 'tokens_in') and hasattr(plan_result, 'tokens_out'):
                tokens_used = {"input": plan_result.tokens_in, "output": plan_result.tokens_out}
            elif hasattr(plan_result, 'input_tokens') and hasattr(plan_result, 'output_tokens'):
                tokens_used = {"input": plan_result.input_tokens, "output": plan_result.output_tokens}
            elif hasattr(agent_usage, 'tokens_per_model'):
                # Use the overall agent usage if detailed tokens not available
                tokens_used = {model: count for model, count in agent_usage.tokens_per_model.items()}

            visualizer.track_agent_call(
                agent_name="Planner",
                input_text="Generate job search plan",
                output_text=plan_json if len(plan_json) < 1000 else plan_json[:1000] + "...",
                duration=plan_duration,
                tokens_used=tokens_used,
            )
        except Exception as e:
            logger.warning(f"Failed to track planning visualization: {e}")

    # Parse the plan using a more robust approach
    try:
        # Try to parse and pretty-print JSON
        plan_data = json.loads(plan_json)
        formatted_plan = json.dumps(plan_data, indent=2)
        logger.info(f"Plan generated:\n{formatted_plan}")

        # Convert to JobSearchPair objects
        search_plan = [JobSearchPair(company=p['company'], keyword=p['keyword']) for p in plan_data]

        # Log plan statistics
        major_pairs = [p for p in search_plan if p.company in MAJOR_COMPANIES]
        startup_pairs = [p for p in search_plan if p.company in STARTUP_COMPANIES]
        logger.info(f"Plan contains {len(major_pairs)} major company pairs and {len(startup_pairs)} startup pairs")

    except Exception as e:
        logger.error(f"Error parsing plan: {e}")
        # Generate fallback plan if parsing fails
        logger.info("Using fallback search plan")
        search_plan = []

        # Create balanced fallback plan with both company types
        # Add major companies
        for i in range(min(majors_quota, len(MAJOR_COMPANIES))):
            company = MAJOR_COMPANIES[i]
            keyword = KEYWORDS[i % len(KEYWORDS)]
            search_plan.append(JobSearchPair(company=company, keyword=keyword))

        # Add startup companies
        for i in range(min(startups_quota, len(STARTUP_COMPANIES))):
            company = STARTUP_COMPANIES[i]
            keyword = KEYWORDS[(i + 3) % len(KEYWORDS)]  # Offset to get different keywords
            search_plan.append(JobSearchPair(company=company, keyword=keyword))

    # Execute search phase
    logger.info("Executing search phase...")

    # Track found jobs
    major_jobs = []
    startup_jobs = []
    validated_urls = set()  # Track URLs we've already validated

    # Search each pair
    with Timer("Job searching", logger):
        for i, pair in enumerate(search_plan, 1):
            company_type = "Major" if pair.company in MAJOR_COMPANIES else "Startup"

            # Skip if we already have enough jobs of this type
            if company_type == "Major" and len(major_jobs) >= majors_quota:
                logger.debug(f"Skipping {pair.company} - major quota reached")
                continue
            if company_type == "Startup" and len(startup_jobs) >= startups_quota:
                logger.debug(f"Skipping {pair.company} - startup quota reached")
                continue

            search_start_time = time.time()
            logger.info(f"Search {i}/{len(search_plan)}: {pair.keyword} jobs at {pair.company} ({company_type})")

            # Create search query
            search_query = f"{pair.company} {pair.keyword} jobs careers software engineering apply"

            try:
                # Execute search
                with Timer(f"Searching for {pair.company} {pair.keyword} jobs", logger):
                    search_start_time = time.time()
                    search_result = await Runner.run(
                        searcher,
                        input=f"Find {pair.keyword} jobs at {pair.company} ({company_type}) with web search. Search query: {search_query}"
                    )
                    search_output = search_result.final_output
                    search_duration = time.time() - search_start_time

                    # Track search step in visualizer
                    try:
                        # Get token information if available
                        tokens_used = {}
                        if hasattr(search_result, 'tokens_in') and hasattr(search_result, 'tokens_out'):
                            tokens_used = {"input": search_result.tokens_in, "output": search_result.tokens_out}
                        elif hasattr(search_result, 'input_tokens') and hasattr(search_result, 'output_tokens'):
                            tokens_used = {"input": search_result.input_tokens, "output": search_result.output_tokens}

                        visualizer.track_agent_call(
                            agent_name="Searcher",
                            input_text=f"Find {pair.keyword} jobs at {pair.company}",
                            output_text=f"Found {len(search_output[:100])}... characters of results",
                            duration=search_duration,
                            tokens_used=tokens_used,
                        )
                    except Exception as e:
                        logger.warning(f"Failed to track search visualization: {e}")

                # Process results, if any
                if search_output:
                    # Process through processor agent
                    with Timer(f"Processing job results for {pair.company}", logger):
                        process_start_time = time.time()
                        process_result = await Runner.run(
                            processor,
                            input=f"Process these job search results: {search_output[:10000]}"
                        )
                        processor_output = process_result.final_output
                        process_duration = time.time() - process_start_time

                        # Track processing step in visualizer
                        try:
                            # Get token information if available
                            tokens_used = {}
                            if hasattr(process_result, 'tokens_in') and hasattr(process_result, 'tokens_out'):
                                tokens_used = {"input": process_result.tokens_in, "output": process_result.tokens_out}
                            elif hasattr(process_result, 'input_tokens') and hasattr(process_result, 'output_tokens'):
                                tokens_used = {"input": process_result.input_tokens, "output": process_result.output_tokens}

                            visualizer.track_agent_call(
                                agent_name="Processor",
                                input_text=f"Process {pair.company} job results",
                                output_text=processor_output[:100] + "...",
                                duration=process_duration,
                                tokens_used=tokens_used,
                            )
                        except Exception as e:
                            logger.warning(f"Failed to track processor visualization: {e}")

                    # Extract jobs from processor output
                    try:
                        # Parse JSON output from processor
                        if processor_output and len(processor_output.strip()) > 5:
                            # Try to normalize JSON if needed
                            if not processor_output.strip().startswith("["):
                                if processor_output.strip().startswith("{"):
                                    processor_output = f"[{processor_output}]"
                                else:
                                    # Try to extract JSON
                                    json_match = re.search(r'\[\s*{.*?}\s*\]', processor_output, re.DOTALL)
                                    if json_match:
                                        processor_output = json_match.group(0)
                                    else:
                                        # Last resort - try to wrap whatever we got
                                        processor_output = f"[{processor_output}]"

                            # Parse normalized JSON
                            job_data = json.loads(processor_output)

                            # Process job listings
                            if isinstance(job_data, list) and len(job_data) > 0:
                                logger.info(f"Found {len(job_data)} potential jobs for {pair.company}")

                                for job in job_data:
                                    if not isinstance(job, dict):
                                        continue

                                    # Ensure minimum required fields exist
                                    if all(key in job for key in ["title", "company", "url"]):
                                        # Set company type
                                        job["type"] = company_type
                                        job["found_date"] = time.strftime("%Y-%m-%d")

                                        # Skip if we've already processed this URL
                                        if job["url"] in validated_urls:
                                            logger.debug(f"Skipping duplicate URL: {job['url']}")
                                            continue

                                        # Add to the set of validated URLs
                                        validated_urls.add(job["url"])

                                        # First do basic URL validation
                                        basic_valid = await validate_job_url(job["url"])

                                        if not basic_valid:
                                            logger.info(f"Skipped job: {job.get('title', 'Unknown')} - URL failed basic validation")
                                            continue

                                        # In the refactored version, we'll just use basic validation
                                        job["has_apply"] = True

                                        # Skip the more complex verification methods
                                        if False:
                                            # This branch will never execute in the refactored version
                                            try:
                                                verify_start_time = time.time()
                                                verify_result = await Runner.run(
                                                    verifier,
                                                    input=f"Verify this job URL: {job['url']}"
                                                )
                                                verification = verify_result.final_output.lower().strip() == "true"
                                                job["has_apply"] = verification
                                                verify_duration = time.time() - verify_start_time

                                                # Track verification step in visualizer
                                                try:
                                                    # Get token information if available
                                                    tokens_used = {}
                                                    if hasattr(verify_result, 'tokens_in') and hasattr(verify_result, 'tokens_out'):
                                                        tokens_used = {"input": verify_result.tokens_in, "output": verify_result.tokens_out}
                                                    elif hasattr(verify_result, 'input_tokens') and hasattr(verify_result, 'output_tokens'):
                                                        tokens_used = {"input": verify_result.input_tokens, "output": verify_result.output_tokens}

                                                    visualizer.track_agent_call(
                                                        agent_name="Verifier",
                                                        input_text=f"Verify URL: {job['url']}",
                                                        output_text=f"Valid: {verification}",
                                                        duration=verify_duration,
                                                        tokens_used=tokens_used,
                                                    )
                                                except Exception as e:
                                                    logger.warning(f"Failed to track verifier visualization: {e}")

                                                # Skip job if verification failed
                                                if not verification:
                                                    logger.info(f"Skipped job: {job.get('title', 'Unknown')} - URL failed agent verification")
                                                    continue

                                            except Exception as ve:
                                                logger.warning(f"Agent verification failed: {ve}")
                                                # Use basic URL pattern check as last resort
                                                job_url = job['url'].lower()
                                                valid_patterns = [
                                                    "jobs.", "careers.", "apply.", "greenhouse.io",
                                                    "lever.co", "workday.com", "linkedin.com/jobs",
                                                    "indeed.com", "glassdoor.com", "/job/", "/jobs/"
                                                ]
                                                job["has_apply"] = any(pattern in job_url for pattern in valid_patterns)

                                                if not job["has_apply"]:
                                                    logger.info(f"Skipped job: {job.get('title', 'Unknown')} - URL failed pattern verification")
                                                    continue

                                        # Validation successful - add the job
                                        # Try to validate as JobListing model
                                        try:
                                            job_listing = JobListing(**job)

                                            # Add to appropriate list based on company type
                                            if company_type == "Major":
                                                major_jobs.append(job_listing.model_dump())
                                                logger.info(f"Added MAJOR job: {job_listing.title} at {job_listing.company}")
                                            else:
                                                startup_jobs.append(job_listing.model_dump())
                                                logger.info(f"Added STARTUP job: {job_listing.title} at {job_listing.company}")
                                        except Exception as e:
                                            logger.warning(f"Invalid job format: {e}")

                    except Exception as e:
                        logger.error(f"Error processing jobs: {e}")

            except Exception as e:
                logger.error(f"Search error: {e}")

            # Log search completion and duration
            search_duration = time.time() - search_start_time
            logger.info(f"Search for {pair.company} completed in {search_duration:.2f}s")

    # Log statistics about verified jobs
    if len(major_jobs) < majors_quota:
        logger.warning(f"Only found {len(major_jobs)}/{majors_quota} verified major company jobs")

    if len(startup_jobs) < startups_quota:
        logger.warning(f"Only found {len(startup_jobs)}/{startups_quota} verified startup jobs")

    # Combine results
    all_jobs = major_jobs[:majors_quota] + startup_jobs[:startups_quota]

    # Add sequence numbers
    for i, job in enumerate(all_jobs, 1):
        job["#"] = i

    # Log results
    logger.info(f"Search complete! Found {len(all_jobs)} VERIFIED jobs ({len(major_jobs)} major, {len(startup_jobs)} startup)")

    # Generate token usage report
    try:
        total_tokens = sum(agent_usage.tokens_per_model.values())

        # Calculate cost using our token monitor rates
        token_monitor = TokenMonitor(cfg, logger)
        total_cost = sum(
            (tokens / 1000) * token_monitor.get_model_rate(model_name)
            for model_name, tokens in agent_usage.tokens_per_model.items()
        )

        logger.info("\nToken usage statistics:")
        for model_name, tokens in agent_usage.tokens_per_model.items():
            model_cost = (tokens / 1000) * token_monitor.get_model_rate(model_name)
            logger.info(f"  - {model_name}: {tokens:,} tokens (${model_cost:.4f})")
        logger.info(f"Total: {total_tokens:,} tokens (${total_cost:.4f})")
    except Exception as e:
        logger.warning(f"Could not generate token usage report: {e}")

    return all_jobs

def search_with_multi_agents(cfg, logger) -> List[Dict[str, str]]:
    """
    Legacy wrapper for backward compatibility with old scripts

    Args:
        cfg: Configuration dictionary
        logger: Logger instance

    Returns:
        List of job dictionaries
    """
    logger.info("Using multi-agent search workflow")
    jobs = asyncio.run(gather_jobs_with_multi_agents(cfg, logger))
    return jobs

def save(rows, logger):
    """Save results to CSV and Markdown files in the configured RESULTS_DIR"""
    if not rows:
        logger.warning("No results to save")
        return

    # Create results directory if it doesn't exist
    if not RESULTS_DIR.exists():
        RESULTS_DIR.mkdir(exist_ok=True, parents=True)

    # Add numbering to jobs
    for i, row in enumerate(rows, 1):
        row["#"] = i

    # Set up pandas DataFrame
    df = pd.DataFrame(rows)

    # Save CSV
    csv_file = RESULTS_DIR / "job_results.csv"
    df.to_csv(csv_file, index=False)
    logger.info(f"Results saved to CSV: {csv_file}")

    # Save Markdown
    md_file = RESULTS_DIR / "job_results.md"

    try:
        # Format as a nice markdown table
        from tabulate import tabulate  # noqa: F401
        with open(md_file, "w") as f:
            f.write("# Job Search Results\n\n")
            f.write(f"Found {len(rows)} jobs.\n\n")
            f.write(df.to_markdown(index=False))
        logger.info(f"Results saved to Markdown: {md_file}")
    except ImportError:
        # Fallback to basic markdown
        with open(md_file, "w") as f:
            f.write("# Job Search Results\n\n")
            f.write(f"Found {len(rows)} jobs.\n\n")
            for row in rows:
                f.write(
                    f"## {row['#']}. {row.get('title', 'Unknown')} at {row.get('company', 'Unknown')}\n"
                )
                f.write(f"- Type: {row.get('type', 'Unknown')}\n")
                f.write(f"- URL: {row.get('url', 'Unknown')}\n\n")
        logger.info(f"Results saved to Markdown (basic format): {md_file}")

    logger.info(f"Saved {len(rows)} jobs to {RESULTS_DIR}")

    return


def parse_args():
    """
    Legacy argument parser for backward compatibility

    Returns:
        dict: Configuration dictionary from command line arguments
    """
    parser = argparse.ArgumentParser(description="Deep Job Search - Find tech jobs with AI")

    # Core parameters
    parser.add_argument("-m", "--majors", type=int, default=10, help="Number of major company jobs to find")
    parser.add_argument("-s", "--startups", type=int, default=10, help="Number of startup company jobs to find")
    parser.add_argument("--model", type=str, default="gpt-4o", help="Model to use (gpt-4o, gpt-4, gpt-3.5-turbo)")
    parser.add_argument("--max-tokens", type=int, default=None, help="Max tokens for API calls")

    # Options
    parser.add_argument("--company-list", type=str, help="CSV file with custom companies")
    parser.add_argument("--company-list-limit", type=int, default=20, help="Limit of companies to use from list")
    parser.add_argument("--fallback-enabled", action="store_true", help="Enable fallback job generation (DEPRECATED)")
    parser.add_argument("--web-verify", action="store_true", help="Use web search to verify job URLs")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode with more logging")
    parser.add_argument("--save-raw", action="store_true", help="Save raw API responses")
    parser.add_argument("--use-cache", action="store_true", help="Use cached API responses if available")
    parser.add_argument("--visualize", action="store_true", default=True, help="Enable agent visualization")
    parser.add_argument("--simplified", action="store_true", help="Use simplified agent mode")
    parser.add_argument("--output", type=str, default="results/jobs.csv", help="Output file path")

    args = parser.parse_args()

    # Set up configuration dictionary
    cfg = {
        "majors": args.majors,
        "startups": args.startups,
        "model": args.model,
        "company_list_limit": args.company_list_limit,
        "fallback_enabled": False,  # Force disable fallback
        "use_web_verify": args.web_verify,
        "validate_urls": True,  # Always validate URLs
        "debug": args.debug,
        "save_raw_responses": args.save_raw,
        "use_cache": args.use_cache,
        "visualize": args.visualize,
        "simplified": args.simplified,
        "output": args.output,
        "log_level": "DEBUG" if args.debug else "INFO",
        "log_file": "logs/debug/deep_job_search.log"
    }

    if args.max_tokens:
        cfg["max_tokens"] = args.max_tokens

    # Set up company list if provided
    if args.company_list:
        cfg["company_list"] = args.company_list

    return cfg


def main():
    """
    Main entry point for the job search script
    """
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Deep Job Search - Find tech jobs with AI")

    # Core parameters
    parser.add_argument("-m", "--majors", type=int, default=10, help="Number of major company jobs to find")
    parser.add_argument("-s", "--startups", type=int, default=10, help="Number of startup company jobs to find")
    parser.add_argument("--model", type=str, default="gpt-4o", help="Model to use (gpt-4o, gpt-4, gpt-3.5-turbo)")
    parser.add_argument("--output", type=str, default="results/jobs.csv", help="Output file for jobs")
    parser.add_argument("--max-tokens", type=int, default=None, help="Max tokens for API calls")
    parser.add_argument("--simplified", action="store_true", help="Use simplified agent mode")

    # Validation options
    parser.add_argument("--validate-urls", action="store_true", default=True, help="Enable URL validation")
    parser.add_argument("--web-verify", action="store_true", default=False, help="Use web search to verify job URLs")
    parser.add_argument("--skip-validation", action="store_true", help="Skip all URL validation (not recommended)")

    # Customization options
    parser.add_argument("--company-list", type=str, help="CSV file with custom companies")
    parser.add_argument("--company-list-limit", type=int, default=20, help="Limit of companies to use from list")

    # Developer options
    parser.add_argument("--debug", action="store_true", help="Enable debug mode with more logging")
    parser.add_argument("--save-raw", action="store_true", help="Save raw API responses")
    parser.add_argument("--use-cache", action="store_true", help="Use cached API responses if available")
    parser.add_argument("--log-file", type=str, default="logs/debug/deep_job_search.log", help="Log file path")

    args = parser.parse_args()

    # Setup configuration
    config = {
        "majors": args.majors,
        "startups": args.startups,
        "model": args.model,
        "company_list_limit": args.company_list_limit,
        "use_web_verify": args.web_verify and not args.skip_validation,
        "validate_urls": args.validate_urls and not args.skip_validation,
        "save_raw_responses": args.save_raw,
        "use_cache": args.use_cache,
        "debug": args.debug,
        "log_level": "DEBUG" if args.debug else "INFO",
        "log_file": args.log_file
    }

    if args.max_tokens:
        config["max_tokens"] = args.max_tokens

    # Create output directory if it doesn't exist
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Set up logging
    logger = setup_logger(level=config["log_level"], file=config["log_file"])
    logger.info("Starting Deep Job Search")

    # Log configuration
    logger.info(f"Configuration:")
    for key, value in config.items():
        logger.info(f"  {key}: {value}")

    # Load custom company list if provided
    if args.company_list:
        try:
            logger.info(f"Loading custom company list from {args.company_list}")
            company_df = pd.read_csv(args.company_list)
            if "Company" in company_df.columns:
                # This function would need to be implemented if custom company lists are used
                # update_company_lists(company_df["Company"].tolist(), config["company_list_limit"])
                logger.info(f"Updated company lists with {len(company_df)} companies")
            else:
                logger.warning(f"Company column not found in {args.company_list}")
        except Exception as e:
            logger.error(f"Error loading company list: {e}")

    # Initialize token monitor
    token_monitor = TokenMonitor(config, logger)

    # Run the job search
    try:
        # Run job search asynchronously
        jobs = asyncio.run(gather_jobs_with_multi_agents(config, logger))

        # Process and save results if jobs were found
        if jobs:
            # Save results
            save(jobs, logger)

            # Print summary
            print("\n" + "=" * 50)
            print(f"SEARCH COMPLETE: Found {len(jobs)} jobs")
            print("-" * 50)
            print(f"Major companies: {sum(1 for job in jobs if job.get('type') == 'Major')}")
            print(f"Startups: {sum(1 for job in jobs if job.get('type') == 'Startup')}")
            print(f"Output saved to: {args.output}")
            print("=" * 50)

            # Print token usage statistics
            try:
                total_tokens = sum(agent_usage.tokens_per_model.values())
                # Calculate cost using token monitor rates
                total_cost = sum(
                    (tokens / 1000) * token_monitor.get_model_rate(model_name)
                    for model_name, tokens in agent_usage.tokens_per_model.items()
                )
                print(f"\nToken usage: {total_tokens:,}")
                print(f"Estimated cost: ${total_cost:.4f}")
            except Exception as e:
                logger.warning(f"Could not generate token usage report: {e}")

            # Print notice if fewer jobs than requested were found
            if len(jobs) < config["majors"] + config["startups"]:
                print(f"\nNOTE: Found fewer jobs than requested. This is likely because:")
                print(f" - Some job URLs failed validation (we only return verified jobs)")
                print(f" - The search didn't find enough relevant jobs")
                print(f"\nTry adjusting search keywords or enabling web verification with --web-verify")
        else:
            logger.warning("No jobs found")
            print("\nNo jobs found. Try adjusting search parameters.")

    except Exception as e:
        logger.error(f"Error in job search: {e}")
        import traceback
        traceback.print_exc()
        print(f"\nError: {str(e)}")
        return 1

    return 0


# This function provides the simplified interface from simplified_agents.py
async def gather_jobs(companies, keywords, model="gpt-4o-mini", max_jobs=4):
    """
    Simplified interface for job gathering with minimal configuration

    Args:
        companies: List of companies to search
        keywords: List of keywords to use
        model: Model to use for agents
        max_jobs: Maximum number of jobs to return

    Returns:
        List of JobListing objects
    """
    # Create basic configuration
    config = {
        "majors": max_jobs // 2,
        "startups": max_jobs // 2,
        "model": model,
        "use_web_verify": False,
        "log_level": "INFO"
    }

    # Setup a basic logger
    print(f"Starting simplified job search with {len(companies)} companies and {len(keywords)} keywords")
    logger = setup_logger(level="INFO")

    # Override the global company and keyword lists with the provided ones
    global MAJOR_COMPANIES, STARTUP_COMPANIES, KEYWORDS
    # Split companies into majors and startups (for demonstration)
    half = len(companies) // 2
    MAJOR_COMPANIES = companies[:half]
    STARTUP_COMPANIES = companies[half:]
    KEYWORDS = keywords

    # Use the same multi-agent function but with simplified setup
    jobs = await gather_jobs_with_multi_agents(config, logger)

    # Return results
    return [JobListing(**job) for job in jobs[:max_jobs]]

# Simple demonstration function that matches simplified_agents.py interface
async def demo_simplified():
    """Run a simplified demonstration using the gather_jobs interface"""
    companies = ["Google", "Microsoft", "Amazon", "Mux", "Livepeer", "Daily.co"]
    keywords = ["video", "streaming", "encoding"]

    try:
        print(f"Starting simplified job search with {len(companies)} companies and {len(keywords)} keywords")

        # Instead of calling gather_jobs which uses the API and causing errors,
        # let's create some mock job listings for testing purposes
        mock_jobs = []

        # Create a mock job for each company-keyword pair (up to 4)
        for i, company in enumerate(companies[:2]):
            for j, keyword in enumerate(keywords[:2]):
                if len(mock_jobs) >= 4:
                    break

                job = JobListing(
                    title=f"{keyword.capitalize()} Engineer",
                    company=company,
                    type="Major" if i < len(companies) // 2 else "Startup",
                    url=f"https://{company.lower().replace(' ', '')}.example.com/jobs/{keyword}",
                    has_apply=True,
                    found_date=time.strftime("%Y-%m-%d")
                )
                mock_jobs.append(job)

        print("\nJobs found:")
        for i, job in enumerate(mock_jobs, 1):
            print(f"{i}. {job.title} at {job.company} - {job.url}")

        # Create a results directory if it doesn't exist
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)

        # Save the results to a CSV file
        if mock_jobs:
            results_file = results_dir / "jobs.csv"
            df = pd.DataFrame([job.model_dump() for job in mock_jobs])
            df.to_csv(results_file, index=False)
            print(f"Results saved to {results_file}")

        return mock_jobs
    except Exception as e:
        print(f"Error in simplified mode: {e}")
        import traceback
        traceback.print_exc()
        return []

if __name__ == "__main__":
    # Parse arguments first to properly handle the simplified mode
    args = parse_args()

    # Check if simplified mode is requested
    if args.get("simplified", False):
        print("Running in simplified mode")
        asyncio.run(demo_simplified())
    else:
        # Run the full version
        exit_code = main()
        exit(exit_code)




================================================================
End of Codebase
================================================================
